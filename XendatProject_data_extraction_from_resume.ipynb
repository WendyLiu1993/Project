{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "uj-nWvx-HaNr",
        "outputId": "8e279efd-cdae-425d-977f-3cb8f903d324"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c94ffb0f-e4a3-4db7-9080-dc3af7fb227c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c94ffb0f-e4a3-4db7-9080-dc3af7fb227c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Anand_Kishore_Resume_EA.docx to Anand_Kishore_Resume_EA.docx\n",
            "Saving Ankush S. -Xendat Collibra Ranger - India.pdf to Ankush S. -Xendat Collibra Ranger - India.pdf\n",
            "Saving bioinfo.pdf to bioinfo.pdf\n",
            "Saving Daniel_Paige_CV-CDA.docx to Daniel_Paige_CV-CDA.docx\n",
            "Saving Jayalakshmi+Thiagarajan_Recent.pdf to Jayalakshmi+Thiagarajan_Recent.pdf\n",
            "Saving Jennifer Morales (2021).docx to Jennifer Morales (2021).docx\n",
            "Saving JoeNovella_Resume-Vertex.docx to JoeNovella_Resume-Vertex.docx\n",
            "Saving LEB resume 17 AUG 2021.pdf to LEB resume 17 AUG 2021.pdf\n",
            "Saving Meena Motwani Resume 9-2021.doc to Meena Motwani Resume 9-2021.doc\n",
            "Saving Rahul_Parihar_BI_Consultant_Resume.docx to Rahul_Parihar_BI_Consultant_Resume.docx\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Reading Word and PDF\n",
        "!pip install python-docx pdfplumber"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsyS2iKQtHU_",
        "outputId": "0532c6f5-83d2-441d-e109-a9e389e51a2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.0-py3-none-any.whl (239 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/239.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/239.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.6/239.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.0-py3-none-any.whl (56 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/56.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.4/56.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.10.0)\n",
            "Collecting pdfminer.six==20231228 (from pdfplumber)\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (9.4.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.28.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (42.0.5)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.21)\n",
            "Installing collected packages: python-docx, pypdfium2, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20231228 pdfplumber-0.11.0 pypdfium2-4.28.0 python-docx-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 1 list resume file and resume text\n",
        "\n",
        "import docx\n",
        "import pdfplumber\n",
        "import os\n",
        "\n",
        "def extract_text_from_docx(file_path):\n",
        "  doc = docx.Document(file_path)\n",
        "  text = []\n",
        "  for para in doc.paragraphs:\n",
        "    text.append(para.text)\n",
        "  return '\\n'.join(text)\n",
        "\n",
        "def extract_text_from_pdf(file_path):\n",
        "  with pdfplumber.open(file_path) as pdf:\n",
        "    text = \"\"\n",
        "    for page in pdf.pages:\n",
        "      text += page.extract_text()\n",
        "  return text\n",
        "\n",
        "def process_files(directory):\n",
        "  output = []\n",
        "  for file_name in os.listdir(directory):\n",
        "    file_path = os.path.join(directory, file_name)\n",
        "    if file_name.endswith('.docx'):\n",
        "      text = extract_text_from_docx(file_path)\n",
        "      output.append(f'{file_name}\\n{text}')\n",
        "    elif file_name.endswith('.pdf'):\n",
        "      text = extract_text_from_pdf(file_path)\n",
        "      output.append(f'{file_name}\\n{text}')\n",
        "  return output\n"
      ],
      "metadata": {
        "id": "LB4HOQTTtKZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openpyxl import Workbook\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    directory = \"/content/\"\n",
        "    results = process_files(directory)\n",
        "\n",
        "    # Create a new workbook\n",
        "    wb = Workbook()\n",
        "    # Create a new worksheet and name it\n",
        "    ws = wb.active\n",
        "    ws.title = \"RawResume\"\n",
        "\n",
        "    # Write the results to the worksheet\n",
        "    for result in results:\n",
        "        file_name, text = result.split('\\n', 1)  # Split the result into file name and text content\n",
        "        ws.append([file_name, text])  # Append file name and text content as a list\n",
        "        print(result)\n",
        "\n",
        "    # Save the workbook\n",
        "    wb.save(\"Resume Python.xlsx\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqH3UQMR14k2",
        "outputId": "fe45bcc4-fb4b-4014-c8ea-4b97f277a826"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jayalakshmi+Thiagarajan_Recent.pdf\n",
            "Cell: 602-570-0197, Home: 6024456143\n",
            "jayabright@gmail.com\n",
            "Jayalakshmi Thiagarajan\n",
            "PROFILE\n",
            " Jayalakshmi Thiagarajan has 20 years of IT industry experience encompassing a wide range of skill\n",
            "set, roles and industry verticals.\n",
            " Good experience in Banking, ATM, Card, Financial, Loan, Life Insurance and web-based domains.\n",
            "Worked with end client Citicorp, Discover Financial, American Express, Apple, MetLife and BB&T\n",
            "SunTrust.\n",
            " Strong experience in business and Data Analysis, Data Modeling, Machine learning, Python coding,\n",
            "Data Profiling, Data Migration, Data Integration and Metadata Management Services.\n",
            " Support new and existing data and ETL pipelines in and out of data warehouse using Snowflake Cloud\n",
            "snow SQL.\n",
            " Effective data analysis using AWS Amazon Redshift, Kinesis with Lambda functions. Based on strategic\n",
            "directions identify emerging trends and patterns using SageMaker service machine learning algorithms.\n",
            " Strong knowledge on Python, Big data concepts, Hive, HBase, Apache Spark, Scale, Hadoop, APIs REST\n",
            "and SOAP, UNIX Shell, Big data and API automations.\n",
            " Good experience in Data Modeling with expertise in creating Schemas, Dimensions Tables, Physical\n",
            "and Logical Data Modeling.\n",
            " Has good experience in data analysis from complex and high dimensional datasets, both structured\n",
            "and unstructured and applying advanced statistical, data mining and machine-learning techniques.\n",
            " Has used SQL and Store Procedures to assist in executing the maintaining data extracts and data to\n",
            "support strategic analytics.\n",
            " Her experience includes creating innovative automation scripts to speed up the project delivery cycle\n",
            "using the tools. She has extensive knowledge in creating new tools with analysis, design, development\n",
            "and quality checking.\n",
            " She is leading multiple projects as both a technical and project management. Has expertise in quality\n",
            "attributes testing and deliveries.\n",
            " She has experience in defining test scope of work, test plans, test cases, and review testing checklists\n",
            "and creating final closure and summary report.\n",
            " She is good in Functional, Component, Regression, Integration security, stability, scalability,\n",
            "reliability, user acceptance testing of multiple projects in accordance with business and functional\n",
            "requirements.\n",
            " Has good experience on software delivery (Manual, SOA Testing &Automation) and end to end Test\n",
            "management. She must do good coordinates with business and development teams to gather\n",
            "requirements and transfer the same to other teams.\n",
            " She is good in preparing the Plan and project sizing based on the scope of the project. She is working\n",
            "as an onshore point of contact provides support for implementation and customization of the frame\n",
            "work of automation projects as required.\n",
            " She worked for environment setups and test data preparations. As an E2E Test manager, conducts\n",
            "the Risk Rating Analyze sessions for all the Testing Phases starting from Analyze.\n",
            "Page 1 of 11Cell: 602-570-0197, Home: 6024456143\n",
            "jayabright@gmail.com\n",
            "SKILL SET:\n",
            "Industry Domain Expertise Banking, ATM, Card, Financial and web based domains, Manufacturing industry\n",
            "and Health Care Domain, Life Insurance.\n",
            "Management Tool RPM (Rational Portfolio Manager), Microsoft project, CA Clarity, PPM (Project and\n",
            "Portfolio Management). Knowledge of project management activities completed\n",
            "management 35 PDUS from PMI.\n",
            "Processes & Models Models: Waterfall Model, Agile Model\n",
            "Process: IBM’s QMS (Quality Management System), RUP(Rational Unified Process),\n",
            "IBM OPAL (On Demand Process Asset Library)\n",
            "Languages, Internet C, C++, Visual Basis 6.0, HTML, DHTML, XML, Python, CGI, Perl, Active Server\n",
            "Programming Page, Shell Scripting, New Scale tool, Dynamo Case Trakker, Tomcat web server,\n",
            "APIGEE API management tool.\n",
            "RDBMS, Big Data, ETL, Snowflake, AWS Cloud, Oracle 8i, SQL Server, DB2(Mainframes), Hadoop, Hive,\n",
            "Cloud Oozie, Hbase, Spark, Scala, APIs, Big data environment setups. Practicing ML.\n",
            "Defect Tracking Tools Test Director 8.0, Mercury Quality Center 9.0., IBM Rational Clear Quest.\n",
            "Platforms Windows server 2016/2008 R2/ XP/2003/NT/95, UNIX\n",
            "Machine Learning Data Mining: Clustering, Naïve Bayes, KNN, Decision Tree, Random Forest, k\n",
            "Means, Time Series. Text Mining: Information Retrieval, Pattern Recognition,\n",
            "Tagging, Information Extraction, Classification. Regression Analysis: Linear\n",
            "Regression, Logistic Regression, Generalized Linear Models, Additive Models,\n",
            "Model Selection and Cross Validation.\n",
            "Machine Learning libraries Scikit-Learn, Pandas, NumPy, SciPy, SpaCy, Keras, Tensorflow, Plotly, Matplotlib.\n",
            "Quality Control Manual testing, ALM, Quality assurance supports,\n",
            "ITKO LISA, SOUP UI, Rest Client.\n",
            "EDUCATION: Academic & Certification: -\n",
            " MCA (Master of Computer Application), Bharathidasan University, Trichy, First Class.\n",
            " B.Sc. (Bachelor of Science Mathematic), Bharathidasan University, Trichy.\n",
            " BigID Certified Professional\n",
            " AWS Certified Machine Learning – Specialty\n",
            " CSM- Certified scrum master\n",
            "EXPERIENCE:\n",
            " Oct 2013 – Till date, Senior Associate, Cognizant.\n",
            " April 2008 – Oct 2013, Project Manager, IBM India Pvt. Ltd. India\n",
            " July 2007 – March 2008, Delivery Project Lead, Mphasis an EDS\n",
            " April 2006 – June 2007, Consultant, Capgemini Pvt., Ltd.,\n",
            " November 2004 – March 2006, Associate Consultant, Polaris software Lab Ltd.,\n",
            " April 2001 to November 2004, Software test Engineer, Sify Ltd.,\n",
            " November 1999 to January 2001, Programmer, Design Expo Network Pvt., Ltd\n",
            "Page 2 of 11\n",
            "29-Jul-2021Cell: 602-570-0197, Home: 6024456143\n",
            "jayabright@gmail.com\n",
            "PROFESSIONAL EXPERIENCE\n",
            "Branch Banking and Trust – April 2020 to till date - Data Analyst\n",
            "Project Name : Shaw\n",
            "Role : Data analyst and validation of mainframe files\n",
            "Environment : Mainframe, SQL server, SOAP,\n",
            "Project Description\n",
            "Shaw system is the retail loan servicing system that Truist is running. Shaw is supports Retail\n",
            "commercial, leasing and floor plan applications. Truist runs the Installment loans, consumer lending,\n",
            "lines of credit, dealer finance, direct and indirect loans.\n",
            "Roles & Responsibilities:\n",
            " Validating and analyzing source data and data integrations between BB&T and Truist accounts.\n",
            " Work with stakeholders (i.e., Product, Data, Engineering teams) to gather requirements and\n",
            "assist with data-related information, by supporting their data infrastructure needs.\n",
            " Mapping existing loan system with migration of account from Truist.\n",
            " Batch run monitoring and verify the balance and control reports.\n",
            " Identify the attributes which needs for APIs.\n",
            " Make sure the current data extraction using ETL methods.\n",
            " Create API architecture and API Management Platform.\n",
            " Validate package runs and validate store procedures and business rules.\n",
            " Support new and existing data and ETL pipelines in and out of data warehouse using Snowflake Cloud\n",
            "snow SQL.\n",
            " Effective data analysis using AWS Amazon Redshift, Kinesis with Lambda functions. Based on\n",
            "strategic directions identify emerging trends and patterns using SageMaker service machine learning\n",
            "algorithms.\n",
            "Metropolitan Life Insurance - November 2018 to March 2020 - Data Architect, Modeler\n",
            "Project 1 Name : Disability Evolution Program\n",
            "Role : Data Architect and Modeler\n",
            "Environment : Oracle, Unix, HBase, Titan DB, Putty, Hadoop,\n",
            "Hive, Shell script, JIRA, Apache spark, Scala, API\n",
            "Project Description\n",
            "Leverage cloud-based SaaS products to deliver core claims capabilities and analytical capabilities to\n",
            "improve the claim processing. DnA is involved in several tracks and the core track integrates MetLife\n",
            "with FINEOS a Cloud based vendor SaaS case (absence) and claim (STD and LTD) management platform.\n",
            "This platform will supplement and eventually replace the UDS and TAM systems.\n",
            "Roles & Responsibilities:\n",
            " Mapping and module the attributes between FINEOS and Metlife.\n",
            " Data integration between the systems and create joins.\n",
            " Create a pattern for continues integration.\n",
            " Understand and translate business needs into data models supporting long-term solutions.\n",
            " Identify and build logical data models and find the relationships between data fields. Work with\n",
            "the Application Development team to implement data strategies, build data flows and develop\n",
            "conceptual data models.\n",
            "Page 3 of 11Cell: 602-570-0197, Home: 6024456143\n",
            "jayabright@gmail.com\n",
            " Create logical and physical data models using best practices to ensure high data quality and\n",
            "reduced redundancy. Optimize and update logical and physical data models. Maintain\n",
            "conceptual, logical and physical data models along with corresponding metadata.\n",
            " Recommend opportunities for reuse of data models in new environments.\n",
            " Perform reverse engineering of physical data models from databases and SQL scripts. Evaluate\n",
            "data models and physical databases for variances and discrepancies.\n",
            " Validate business data objects for accuracy and completeness. Analyze data-related system\n",
            "integration challenges and propose appropriate solutions. Develop data models according to\n",
            "company standards.\n",
            " Guide System Analysts, Engineers, Programmers and others on project limitations and\n",
            "capabilities, performance requirements and interfaces. Review modifications to existing\n",
            "software to improve efficiency and performance.\n",
            "Project 2 Name : EDMS acceleration – Metlife\n",
            "Role : Data Architect\n",
            "Environment : Unix, HBase, Titan DB, Yarn, Eclipse, Putty, Hadoop, Kafka\n",
            "Hive, MapReduce, Oozie, Shell script, Rally, Apache spark, Scala\n",
            "Project Description\n",
            "EDMS data lake platform acceleration program is a strategic effort to modernize, simplify, and enable greater\n",
            "utilization of data across the enterprise. EDMS is to provide an enterprise data management strategy within\n",
            "the CSC Insight program. Load the One Time and Delta entities and sub entities feeds received from various\n",
            "line of business of SORs. Via APIs provide 360-degree view of the party, customers, policy holder and provider,\n",
            "claims. CSC (Customer Service Consultant) functionalities into a single front end application within the\n",
            "Salesforce Console.\n",
            "Roles & Responsibilities:\n",
            " Active participation for all plans and estimates the capacity.\n",
            " Identify and create physical, logical and relational data from existing system entity and attributes.\n",
            " Collaborate with Business Analysts and Data Scientists to understand data requirements and\n",
            "provide data modeling solutions. Design principles for Data Warehousing techniques ETL and\n",
            "Architecture.\n",
            " Build CICD Data Pipelines using Python. Create End to End data flow architecture.\n",
            " Develop data architectural diagrams to illustrate architectural complexities and interactions.\n",
            " Analyzing and validating the builds and setup the environment.\n",
            " Use hive, HBase shell, API services to do data validation and create patterns.\n",
            " Creating automation framework and creating automation scripts using python and bshell.\n",
            " Worked on environment setups. And end to end business scenarios creation and defined.\n",
            " Provided end to end support for the project.\n",
            "Project 3 Name : Panorama– Metropolitan Life Insurance Company\n",
            "Role : Data Analyst\n",
            "Environment : Unix, HBase, Titan DB, Yarn, Eclipse, Putty, Java, Hadoop, Kafka,\n",
            "Hive, MapReuce, Oozie, Shell script, Rally, Apache spark, Scala\n",
            "Project Description\n",
            "Panorama represents a major effort to re-engineer its U.S. legacy systems infrastructure and processing\n",
            "and migration all admin systems to new DXC system. The Global Data Integration initiative focuses on\n",
            "improving the end-to-end control, ability to reconcile and trace information to and from the point of\n",
            "Page 4 of 11\n",
            "29-Jul-2021Cell: 602-570-0197, Home: 6024456143\n",
            "jayabright@gmail.com\n",
            "transaction to the General Ledger, data quality, timeliness and consistency of financial information by\n",
            "standardizing transactional data and implementing consistent accounting processes\n",
            "Roles & Responsibilities:\n",
            " Primarily responsible for developing strong collaborative working relationships with Client and\n",
            "other business partners.\n",
            " Responsible for business scenarios analysis and defining functional specifications and data\n",
            "mining confirmations.\n",
            " Flexible, analytical approach to design, develop, and evaluate predictive models and advanced\n",
            "algorithms that lead to optimal value extraction from the data. Involved in collecting\n",
            "information from the offshore team and presenting it to the client and getting answers from the\n",
            "client and other business partners.\n",
            " Responsible for coordinating with testing team and MetLife’s business and DXC to identify and\n",
            "analyze the gap and offer solution to the client. She also responsible the confirming the business\n",
            "scenarios functional behaviors by validating using WMA application and accounting tables.\n",
            "American Express - April 2016 – June 2017 - Big data analyst\n",
            "Project Name : Credit Risk 2020 – Global Risk Technology -Big Data Capabilities\n",
            "Role : Big Data analyst\n",
            "Environment : Eclipse, Putty, Java, Hadoop, Hive, MapReduce, Oozie,\n",
            "Shell script, Rally\n",
            "Project Description\n",
            "EDP: This capability assists AXP to proactively identify any fraudulent financial activities, credit card\n",
            "transactions, etc. The system will capture credit card transactions and other financial activities and\n",
            "negativity notifications from various sources and calculate positivity and negativity of the elements\n",
            "associated with the transactions based on the predefined business rules. EDP ingests the high volume of\n",
            "transactions and notifications from CS3 contributed by various SORs like GNA, GCAP, CAS, PSB, GCP etc.,\n",
            "calculates the profile-attributes (supports multiple negativity types like Fraud, CBO, Derog, MR Abuse, SPAM\n",
            "etc) and makes the negative data available for consumers via Real-time API (Rest service via APIGEE) and in\n",
            "batch mode via Cornerstone 3.0. Daily Eligibility is a daily running batch job, aims to find the eligibility of the\n",
            "AMEX customers on a daily basis for new product offer or upgrade of existing card members.\n",
            "Roles & Responsibilities:\n",
            " Active participation for all sprints plans and estimate the capacity.\n",
            " Identify the ways through which AMEX solicit offers to its customers like:\n",
            "1. Proactive: AMEX itself proactively offers new/upgrade offers to its customers\n",
            "2. Interactive: Through AMEX online portal / over phone\n",
            "3. Reactive: Customer contacting AMEX, then it will evaluate whether a product can be offered to the\n",
            "customer or not.\n",
            " Analyzing and validating the builds and setup the environment for testing.\n",
            " Using hive, HBase, shell script do data validation. Active participation in Test scope of work,\n",
            "Test strategy, Test plan preparation, Task allocation and monitoring the group, Co-ordinate and\n",
            "manage the schedules of the test team, carries our detailed requirements analysis, prepares and\n",
            "maintain test ware (test cases, test data, test stubs/Harnesses) in auditable form, ensure\n",
            "traceability between requirements and test cases, test metrics collection, creating final closure\n",
            "and summary report.\n",
            " Managing and scheduling the resources in multiple projects and maintained resource utilization\n",
            "details. Do Show and Tell and supports UATs.\n",
            "Page 5 of 11Cell: 602-570-0197, Home: 6024456143\n",
            "jayabright@gmail.com\n",
            " Worked on Daily Eligibility – daily running batch job, aims to find the eligibility of the AMEX\n",
            "customers on a daily basis for new product offer or upgrade of existing card members.\n",
            " Involved in testing of inter-operable fraud risk processes via PI and data services in batch and\n",
            "real time. Provide the support to full spectrum profile (negative and non-negative attributes) of\n",
            "digital data such as IP address, email, phone etc. for fraud risk management across the customer\n",
            "lifecycle, and also supports card and non-card processes.\n",
            " Responsible for all client deliverable. Responsible for risk mitigations and escalation.\n",
            " Support to QA for all quality process improvements. Managing the test releases, qualified\n",
            "releases for production.\n",
            "Aetna Dynamo - Aug 2015 – April 2016 - Data Analyst\n",
            "Project Name : Aetna Dynamo\n",
            "Client : Aetna\n",
            "Role : Data analyst\n",
            "Environment : SQL Server Management Studio, Case Trakker Dynamo 5.5\n",
            "Project Description\n",
            "Dynamo is a Care Management application used by health insurance companies like Aetna to monitor\n",
            "member’s health and to provide care. Each states health plan can be implemented in dynamo system. Both\n",
            "Medicaid and Medicare recipients can be handled in Dynamo. Dynamo can handle Dual members as well.\n",
            "Dynamo also has its auxiliary application like Dynamo Disconnect and Dynamo Web Portal.\n",
            "Roles & Responsibilities:\n",
            " Analyzing and validating the Store Procedures and develop SQL queries for validation.\n",
            " Using Dynamo writer, create automatic store procedure to generate reports for A&G reports validation.\n",
            " Active participation in project planning activities.\n",
            " Active participation in Project plane preparation, project estimations, team building activities and\n",
            "domain, testing knowledge sharing.\n",
            " Active participation in Test scope of work, Test strategy, Test plan preparation, Task allocation and\n",
            "monitoring the group, Co-ordinate and manage the schedules of the test team, carries our detailed\n",
            "requirements analysis, prepares and maintain test ware (test cases, test data, test stubs/Harnesses) in\n",
            "auditable form, ensure traceability between requirements and test cases, test metrics collection,\n",
            "creating final closure and summary report.\n",
            " Managing and scheduling the resources in multiple projects and maintained resource utilization details.\n",
            " Responsible for all client deliverable. Responsible for risk mitigations and escalation.\n",
            " Worked on ‘Dynamo Config’ tool which can construct the dynamo application as per the business\n",
            "requirements which differ from state to state and plan to plan.\n",
            " Supported Dynamo Disconnect Application, an offline system which store all the members’ information\n",
            "captured in local and it can synchronize it later with Dynamo.\n",
            " Involved in testing the Dynamo web portal system which is by member portal and provider portal users.\n",
            "Apple/American Express - Oct 2013 – Sep 2014 - API Automation, Data Manager\n",
            "Project 1 Name : Mobile Payments and Engineering-iPhone\n",
            "Client : American Express & Apple\n",
            "Environment : QC 11.5, MPP, SOAP UI, REST Client, HP Service Test\n",
            "Project Description\n",
            "Yankee is a mobile payment service that lets certain Apple mobile devices and Samsung galaxy make\n",
            "payments at retail and online checkout. It intends to digitize and replace the credit or debit magnetic stripe\n",
            "card transaction at credit card terminals. The service lets Apple devices wirelessly communicate with point\n",
            "Page 6 of 11\n",
            "29-Jul-2021Cell: 602-570-0197, Home: 6024456143\n",
            "jayabright@gmail.com\n",
            "of sale systems using a near field communication (NFC) antenna, a \"dedicated chip that stores encrypted\n",
            "payment information\" (known as the Secure Element), and Apple's Touch ID and Passbook.\n",
            "Roles & Responsibilities:\n",
            " Work for Environment stability testing for continues delivery module.\n",
            " Work for data managements for vendors and internal testing.\n",
            " Involving testing activities and deliveries.\n",
            " Did interface testing with Apple.\n",
            " Assisting PMs and project team to develop and maintain Project Plans, Schedules, Resource Plans and\n",
            "Budgets and to develop project initiation documents.\n",
            " Preparing, maintaining and submitting project/portfolio reports with Leadership Team, Senior\n",
            "Management and Clients. Contributing to and review key project documentation, weekly delivery status\n",
            "and operational parameters.\n",
            " Works closely with Project Managers to maintain risk and issue registers at the project levels. Driving\n",
            "the identification and adoption of project management best practices.\n",
            " Enabling project teams to be compliant with corporate project methodologies and processes.\n",
            " Initiate call for validating the End to End business flows by coordinating with all the impacted\n",
            "Application, Interfacing, Backend, and Testing and Business/Partner teams.\n",
            " Prepare clear documentation on the project activities and E2E test report with all the details and initiate\n",
            "for the signoff.\n",
            " Create automation frame work and create scripts using python.\n",
            "Project 2 Name : Digital Enterprise Quality Assurance.\n",
            "Client : American Express\n",
            "Environment : .Net, Oracle, CMDB, Application Inventory Management. Python\n",
            "Project Description\n",
            "American Express Company is a global travel, financial and network services provider. Founded in 1850, the\n",
            "Company serves individuals with charge and credit cards, Travelers Cheques and other stored value products.\n",
            "It also offers financial planning, brokerage services, and mutual funds, insurance and other investment\n",
            "products. Through its family of Corporate Card services, American Express helps companies and institutions\n",
            "manage their travel, entertainment and purchasing expenses. DEQA – offers its services to American Express\n",
            "projects in all phases of testing. IB application is the AMEX application facilitating the customer to view their\n",
            "Bonus earned in brief. This application provides the one stop solution to provide all the features for AMEX\n",
            "card member to manage all his activities.\n",
            "Roles & Responsibilities:\n",
            " In project initiation phase, active participation in project planning activities like scope, resources,\n",
            "establishing schedule, defining Business case, creating plans & identifying reporting requirements.\n",
            " Responsible for entering high level project information, to get a Project Number assigned, to create\n",
            "wherever applicable appropriate parent/child project hierarchy.\n",
            " Estimation, resource planning and tracking; team formation and allocation, milestones and\n",
            "dependencies risk mitigation, metrics tracking, auditing and reporting, and reviews. Dedicating in\n",
            "maintaining high quality standards and establishing processes. Automate the test data process using\n",
            "TDIM tool.\n",
            "American Express - April 2008 – Oct 2013 - Project Manager\n",
            "Project Name : IT Service Catalog, ITSM\n",
            "Client : American Express\n",
            "Role : Technical Lead\n",
            "Environment : JSF Front end & New Scale 9.0, Oracle 8i, CMDB\n",
            "Project Description\n",
            "Page 7 of 11Cell: 602-570-0197, Home: 6024456143\n",
            "jayabright@gmail.com\n",
            "Service Catalog is the one stop shop of all IT services at American Express. Service Catalog facilitated end to\n",
            "end service provisioning through a global requests tool that empowers customers with choices, speed and\n",
            "flexibility, and superior experience. Service Catalogue forms the foundation for secure and traceable IT\n",
            "services. Services are provisioned and fulfilled through various request structures inbuilt internally within\n",
            "IT Service Catalog and externally in the supplier alignment domain. It Service Catalog interfaces with a\n",
            "number of external service providers comprising all processes required to manage the outsourced services\n",
            "and to deliver premium value to AXP. Provide metrics to enable the organization to measure and manage\n",
            "Customer Satisfaction, service delivery performance, and enhance the IT Service Catalog efficiency and\n",
            "effectiveness.\n",
            "Roles & Responsibilities:\n",
            " In project initiation phase, active participation in project planning activities like scope, resources,\n",
            "establishing schedule, defining Business case, creating plans & identifying reporting requirements.\n",
            " Responsible for entering high level project information, to get a Project Number assigned, to create\n",
            "wherever applicable appropriate parent/child project hierarchy.\n",
            " Active participation in Test scope of work, Test strategy, Test plan preparation, Task allocation and\n",
            "monitoring the group, Co-ordinate and manage the schedules of the test team, carries our detailed\n",
            "requirements analysis, prepares and maintain test ware like test cases, test data, test stubs/Harnesses in\n",
            "auditable form, ensure traceability between requirements and test cases, test metrics collection,\n",
            "creating final closure and summary report.\n",
            " Create the automation scripts for the project work and execute the automation scripts for functional and\n",
            "performance of the project.\n",
            " Responsible for managing Project Change Controls so that they don’t adversely impact the project.\n",
            " Manage effective communication Plan such that all stakeholders are aware of new or modified services\n",
            " Managing the scheduling the resources in multiple projects and maintained resource utilization details.\n",
            "Support to Quality Assurance for all quality process improvements. Managing the test releases, qualified\n",
            "releases for productions.\n",
            " Managed the Project using RPM (Rational Portfolio Manager) tool and would be able to use the tool to\n",
            "analyze the work plan and provide assessment.\n",
            " Work with various teams to make sure the system interfaces like ITSM, SRS, HP-EDB, CMDB, AIM.\n",
            " Ensure smooth and defect free User acceptance test of the Services Deployed for every release.\n",
            " Involving the Auditing and metrics tracking.\n",
            " Facilitate and Conduct Root cause analysis sessions and document corrective and preventive actions\n",
            " Solving team technical doubts and queries. Daily delivery reporting and involving the client meetings.\n",
            "Verify and approve Project Resource Timesheets.\n",
            "General Motors - July 2007 – March 2008 - Delivery Project Lead\n",
            "Project Name : General Motors German –Insurance\n",
            "Client : General Motors\n",
            "Environment : Oracle, XML, Java\n",
            "Project Description\n",
            "GMAC is a global financial services company. Provide automotive finance products and services. GMAC has\n",
            "expanded its business to include Mortgage Operations, Insurance, and Commercial Finance & Online\n",
            "Banking.\n",
            "Roles & Responsibilities:\n",
            " Leading 49-member team. Involved in delivering offshore QC business, estimation, planning and\n",
            "tracking, WBS, team formation, resource allocation, milestones and dependencies risk mitigation, test\n",
            "case management, defect management, system integration tests, test report generation, escalation,\n",
            "service level agreement compliance, metrics tracking/auditing and reporting, reviews, personnel\n",
            "management.\n",
            "Page 8 of 11\n",
            "29-Jul-2021Cell: 602-570-0197, Home: 6024456143\n",
            "jayabright@gmail.com\n",
            " Ensures traceability matrix between requirements and test cases, Test metrics collation, Inputs to final\n",
            "summary Report. Task allocation and monitoring the group, Co-ordinate and mange the schedules of\n",
            "the test team. Planning for knowledge sharing sessions\n",
            "Discover Financial Services - - April 2006 – June 2007 - Project Lead\n",
            "Project 1 Name : E-commerce DFS\n",
            "Client : Discover Financial Services\n",
            "Environment : Java, J2EE, DB2 and Windows 2000.\n",
            "Project Description\n",
            "The project deals with E-commerce perspective (Web Based) in which entire online functionalities covers in\n",
            "Account center. Discover card is designed for complete online services of all the financial transactions in the\n",
            "Account center. The internal process in Account center mainly used for the customers applications related to\n",
            "Card Registration, Online Payments, Balance Transfers, Shop Discover, Protection Solutions, Online\n",
            "Statements, Pay utilities, Auto Pay and etc., and the discover Products mainly deals with Credit Cards, Small\n",
            "Business Cards, Gift Cards, Loans, Savings, Insurance, Protection Solutions. This project deals with various\n",
            "modules such as Miles Enhancements Phase II, Terms Level 19,Credit score Tracker, Near Prime, Virtual e-\n",
            "Gift cards, Product Enhancements, Public site Redesign, Private Site Redesign. All the modules are related to\n",
            "new functionalities for all types of Card Types, i.e. Consumer cards, Business Cards, Corporate Cards.\n",
            "Roles & Responsibilities:\n",
            " Leading a Challenging role with a team of 32 Members.\n",
            " Assigning Tasks to Team Members. Test Data Preparation using Various Production Assurance Tools.\n",
            " Reporting the bugs using Quality Centre 9.2. Participating in preparing, Requirement’s analysis, Scenario\n",
            "Identifications, Test cases, Test data Preparations, Execution status, Test Summary reports, Walk\n",
            "Through, Review meetings.\n",
            " Participating in delivering Quality Deliverables. (Including projects and Ongoing Projects.), Participating\n",
            "in status Meetings, (Weekly & Monthly.). Involving in preparing Dashboard.\n",
            " Clarifying the doubts to various teams regarding domain perspective, functionality and Various Tools\n",
            "Perspective. Interacting with Onsite coordinators & Clients whenever necessary.\n",
            " Performing Production Testing. (On call support with Clients and Various development groups)\n",
            " Attending sessions related to projects, Domain wise and new tools. Involved in Project management\n",
            "activities such as maintaining all the project related documents. Involved in Command Centre activities\n",
            "for connecting to different applications and accessing various tools in Remote Desktop.\n",
            "Project 2 Name : Universal Domestic Acceptance Discover, US -Card\n",
            "Client : Discover Financial Services\n",
            "Environment : Java, J2EE, DB2 and Windows 2000.\n",
            "Project Description\n",
            "WAS Tuning involves running the application under load to gather statistics and obtain the optimal Web\n",
            "Sphere Environment settings for the application. Another goal is to start all the Web Sphere applications\n",
            "running under load and then add the new application to the mix to test its performance. As each application\n",
            "comes up, it will be WAS Tuned with a representation of the current production load, this will provide\n",
            "statistics on that application and to see the impact of that new application on the environment. This is\n",
            "directed at volume testing of the application in Web Sphere and not functional verification. There were 200+\n",
            "applications where we support performance tuning for protocols like Web/client server protocol and\n",
            "Java/RMI based applications protocol.\n",
            "Roles & Responsibilities:\n",
            " Leading 41-member team. Active participation in project planning activities.\n",
            "Page 9 of 11Cell: 602-570-0197, Home: 6024456143\n",
            "jayabright@gmail.com\n",
            " Active participation in Project plane preparation, project estimations, team building activities and\n",
            "domain, testing knowledge sharing.\n",
            " Active participation in Test scope of work, Test strategy, Test plan preparation, Task allocation and\n",
            "monitoring the group, Co-ordinate and manage the schedules of the test team, carries our detailed\n",
            "requirements analysis, prepares and maintain test ware (test cases, test data, test stubs/Harnesses) in\n",
            "auditable form, ensure traceability between requirements and test cases, test metrics collection,\n",
            "creating final closure and summary report.\n",
            " Managing and scheduling the resources in multiple projects and maintained resource utilization details.\n",
            " Responsible for all client deliverable. Responsible for risk mitigations and escalation.\n",
            " Support to QA for all quality process improvements. Managing the test releases, qualified release for\n",
            "production.\n",
            "Citicorp development Center , HDFC Bank - November 2004 – March 2006 - Technical Lead\n",
            "Project 1 Name : Global Cat\n",
            "Client : Citicorp Development Center, LosAngeles\n",
            "Environment : CitiTest developed in Visual Basic and MS-Access & STIM,\n",
            "IBM-Card simulator.\n",
            "Project Description\n",
            "The main objective of this project is to support delivery of the GCAT software to deploying countries, while\n",
            "bringing up all existing Global CAT countries to the current release. Additional features to the previous\n",
            "Global CAT release and support for a new Diebold model, the Opteva is provided as part of this release. This\n",
            "release will also use SSL to streamline the secure communication between ATM and the Remote Operator\n",
            "Interface (ROI), and will eliminate the need for CA server and VPN. This release is targeted to be deployed in\n",
            "new businesses Philippines, Poland, Thailand, and Guam along with existing businesses US, Indonesia,\n",
            "Malaysia, Taiwan, Belgium, Greece, Spain, United Kingdom, Brazil and Germany. Taiwan business will\n",
            "migrate from WinNT to XP OS. This release will have support for the CAT2E, Diebold, NCR, and Opteva\n",
            "platforms including VIP support on all four platforms for the U.S. business.\n",
            "Roles & Responsibilities:\n",
            " Technically lead 54-member team\n",
            " Understanding and verifying Requirement Documents. Clarify testing team technical doubts.\n",
            " Planning for ATM testing methods to cover the requirements.\n",
            " Preparing and Executing Test Cases based on the Functional Specification/Requirement Analysis\n",
            "document provided for the application and writing test cases by evaluating the application.\n",
            " Test case Updating & Review , Security Testing\n",
            " Defects/Issues Analysis and Tracking, STR Mapping & Test Results Updating, Planning, Tracking, and\n",
            "Managing Test Environment Setup Activities, Send Daily status Report, Weekly Report & Monthly\n",
            "Report.\n",
            "Project 2 Name : Credit Origination\n",
            "Client : HDFC\n",
            "Environment : Java, JSP, Java Servlets, and Oracle8i, Web logic Server 7.1.\n",
            "With N-tier architecture and thin client application\n",
            "Project Description\n",
            "Intellect Origination is a web-based product intended to meet the lending requirements, acquiring more\n",
            "customers, streamlining the process flow of loan applications and enhance the profitability from customer\n",
            "relationships of Banks and Financial Institutions. Broadly the product comprises of three modules. They are\n",
            "Lead Management System to manage and track leads, Loan Origination System for application creation to\n",
            "Page 10 of 11\n",
            "29-Jul-2021Cell: 602-570-0197, Home: 6024456143\n",
            "jayabright@gmail.com\n",
            "deal closure and Credit Decision Engine for decision in completely configurable manner. This product\n",
            "interfaces with Intellect Lending for Loan booking, Intellect Reports for report generation, Customer\n",
            "communication for letter generation and Intellect Core for Customer Identification (CIF) number.\n",
            "Roles & Responsibilities:\n",
            " Preparation of System Test Plan, Set up of test environment.\n",
            " Preparing Functional, system, UAT Test Cases, Test Data Preparation. Test case Execution and\n",
            "Reporting Defects, Documentation testing. Inputs to final summary Reports.\n",
            "Sift Ltd., April 2001 to Nov 2004 Software test Engineer\n",
            "Project Name : Web based various projects\n",
            "Environment : Windows NT, Java Script, Java Applet, .NET, ASP, IIS.\n",
            "Project Description\n",
            "Project Type: Web related projects and the project are Workflow Inner Space, J & J SFA, SRF, Account\n",
            "Structure- Randox UK, Alpha Bug Reporter, Orange World.\n",
            "Roles & Responsibilities:\n",
            " Involving project and product-based testing activities. As a module lead sharing the domain, testing\n",
            "knowledge\n",
            " Preparation of System Test Plan, Set up of test environment. Preparing Functional, system, UAT Test\n",
            "Cases, Test Data Preparation. Test case Execution and Reporting Defects, Documentation testing. Inputs\n",
            "to final summary Reports. Preparing End to End test cases and execute. Involving client UAT testing.\n",
            "Involving quality-improving process.\n",
            " Involving CMM level QA activities.\n",
            "Design Expo Network Pvt., Ltd - Nov 1999 to Jan 2001 - Software Programmer\n",
            "Project Name : Web based developments\n",
            "Role : Software Programmer\n",
            "Environment : Windows NT, Java Script, Java Applet, CGI, ASP, IIS,\n",
            "HTML, SQL Server.\n",
            "Project Description\n",
            "Web based developments and the projects are Shubhyatra.com, Mobile.Shubhyatra.com, Crick live.com.\n",
            "www.sawaal.com is one of the popular information portals with communication and transactions which also\n",
            "contains personalized services about Horoscope, TV Listings, Personalized News, Portfolio Tracker, Market\n",
            "Watch, Weather, Calendar, Chat, Download Software, Currency Converter, Personal Home Page etc., Search\n",
            "Engine for Art/Humanities, Education, Entertainment, Travel, Sports, Science, Business and News.\n",
            "Roles & Responsibilities:\n",
            " Understanding Requirement Documents. Understanding HLD and create coding based on the\n",
            "requirements.\n",
            " Involving Code reviews. Involving Unit test case preparation and testing. Design Database and Create\n",
            "data base structures.\n",
            "Page 11 of 11\n",
            "LEB resume 17 AUG 2021.pdf\n",
            "Laura Blabac, MS, RHIA, CHDA\n",
            "7682 Teal Road, Woodbury, MN 55125 ● 651-587-4415 ● leblabac2172@mac.com\n",
            "Professional OBJECTIVE\n",
            "To work in a position where my experience in health information technology and passion for optimization,\n",
            "analytics and interoperability will add strategic value to an organization.\n",
            "Skill Highlights\n",
            "Clinical terminologies Data management\n",
            "Standards specifications (HL7, FHIR, 837) Project management including Agile\n",
            "Data analytics Writing, publication and design skills\n",
            "Technical implementations Teaching, public speaking\n",
            "PROFESSIONAL EXPERIENCE\n",
            "Senior Capability Manager, UnitedHealth Group, Global Research & Development, 2017– present\n",
            " Assess and map clinical and business terms and codes for proprietary ontology\n",
            " Co-lead design efforts and feature development for individual health record (IHR)\n",
            " Identify ontology integration opportunities and perform ontology modeling\n",
            " Facilitate interface development for IHR product\n",
            " Designed and FHIR-mapped a 12-class clinical data API schema consumed by multiple customers\n",
            " Serve as Product Owner for IHR API\n",
            " Facilitate ontology governance and ensure documentation creation\n",
            "Senior Consultant, Prism Healthcare Partners, LTD, 2016 – 2017\n",
            " Assess revenue cycle management workflows within healthcare organizations\n",
            " Develop/implement productivity monitoring dashboards using Tableau/Access solution\n",
            " Lead implementation of technology solutions to support clinical information flow\n",
            "Business Analyst (associate-2010, intermediate-2011, senior-2013), Allina Health, 2010-2016\n",
            " EMRM Council support/analyst\n",
            " Project lead/analyst for organizational EHR provider data management project (data cleanse, data\n",
            "capture improvement)\n",
            " Project lead/analyst for system integration supporting release of information department\n",
            " Project lead/analyst for custom organizational EHR clinical data map and disclosure report repository;\n",
            "created database and supporting report dictionaries\n",
            " Project lead and report analyst for implementation of functionality supporting Meaningful Use Stage 1 &\n",
            "2 core objectives related to access and disclosure of electronic records and transitions of care, Program\n",
            "group member\n",
            " Develop standardized reports for records release\n",
            " Managed HIM department SharePoint site launch, designed/implemented functional team sites\n",
            " Designed/facilitated & published HIM Intranet site\n",
            " Developed deficiency record, deficiency report and release of information report/print group data\n",
            "dictionaries\n",
            "COMPUTER APPLICATIONS/PROGRAMS\n",
            "Intermediate/Advanced MS Office (Word, Excel, PowerPoint, Access, Visio, Project), SharePoint, Tableau 10.5,\n",
            "ServiceNow, PeopleSoft, SPSS, SQL, JIRA, Rally, Protégé, EMR Applications: Epic, Allscripts, Cerner, Hyland\n",
            "OnBaseTeaching/COURSE DEVELOPMENT Experience\n",
            "Adjunct Faculty (Health Information Technology), Indiana Technical College, 2013 – 2015\n",
            "Adjunct Faculty (Theology), University of St. Thomas, 1999 – 2005\n",
            "Courses developed: Pathophysiology & Pharmacology for HI Professionals, HIT Project Management, Health\n",
            "Information Privacy & Security, Christian Theological Tradition, Old Testament/Hebrew Scriptures\n",
            "Education\n",
            "College of St. Scholastica, MS, Health Information Management/IT Leadership, 2012\n",
            "College of St. Scholastica, Certificate, Health Informatics, 2012\n",
            "Rasmussen College, AAS, Health Information Technology, 2005\n",
            "St. John’s University, MA, Theology, 1997\n",
            "Mercyhurst College, BA, Dance (Magna cum Laude), 1994\n",
            "Professional CREDENTIALS & CERTIFICATIONS\n",
            "Certified Healthcare Data Analyst (CHDA) certified, 2014\n",
            "Registered Health Information Administrator (RHIA) certified, 2012\n",
            "Epic HIM-Deficiency Tracking certified, 2009 - 2014\n",
            "Epic HIM-Release of Information certified, 2009 – 2014\n",
            "Professional ACTIVITIES\n",
            "Memberships\n",
            "Health Information Management Systems Society (HIMSS) 2011 – present\n",
            "American Health Information Management Association (AHIMA) 2004 – present\n",
            "Minnesota Health Information Management Association (MHIMA) 2004 – present\n",
            "ARMA, International 2015 – 2017\n",
            "Presentations\n",
            "On the Road Again: Consulting in HIM MHIMA 2017\n",
            "Advocacy 101 (co-presenter) MHIMA 2016\n",
            "Information Governance Jeopardy (co-presenter) MHIMA 2016\n",
            "EHR Evolution: From Meaningful Use to Meaningfully Using MHIMA 2015\n",
            "Positioning Your Organization for Successful Information Governance (co-presenter) AHIMA 2014\n",
            "Project Management MHIMA Region D 2014\n",
            "Investing in HI: principles & practices for governance of healthcare big data MHIMA 2014\n",
            "Participation\n",
            "Minnesota Health Information Management Association, Region G President Elect 2018 – 2019\n",
            "Minnesota Epic User Group/HIM Special Interest Group Chair 2011 – 2013\n",
            "Minnesota Health Information Management Association\n",
            "MHIMA Past-President 2016 - 2017\n",
            "MHIMA President 2015 - 2016\n",
            "MHIMA President-Elect 2014 - 2015\n",
            "Co-Chair, Marketing & Communications Committee 2010 – 2014\n",
            "Rasmussen College HIT Advisory Board 2009 – present\n",
            "Ankush S. -Xendat Collibra Ranger - India.pdf\n",
            "Ankush S.\n",
            "Data Governance | Collibra Ranger\n",
            "ACADEMIC QUALIFICATION\n",
            "PGDM Finance 2019 Lal Bahadur Shastri Institute of Management, New Delhi 6.95 out of 10\n",
            "B. Tech (Mechanical) 2016 D.A.V Institute of Engineering and Technology, Jalandhar 70.71%\n",
            "Class XII 2012 CBSE 77%\n",
            "Class X 2010 ICSE 78.78%\n",
            "PROFESSIONAL EXPERIENCE\n",
            "Infosys Limited Senior Associate Consultant (Data Governance) July 2020 - Present\n",
            "Associate Consultant (Data Governance) Jun 2019 - Jun 2020\n",
            "PROFESSIONAL EXERIENCE - Summary\n",
            "Responsibilities: -\n",
            " Understanding client’s requirements and organization structure and implementing the same in\n",
            "Collibra\n",
            " Implemented the end-to-end data lineage from data source to consumption using Collibra\n",
            " Created the business glossary to have consistency across the data users in Collibra\n",
            " Operationalized the Data Governance policies\n",
            " Understanding the client processes and implement it using the Collibra workflows\n",
            " Created custom workflows to create/update/remove any metadata using eclipse\n",
            " Created workflows to generate excel based reports from Collibra DGC\n",
            "US Financial\n",
            " Created user persona specific Views, Lineage Diagrams\n",
            "Institution\n",
            " Created user persona specific Dashboards for ease of navigating and using important\n",
            "Collibra DGC\n",
            "functionalities\n",
            "Implementation\n",
            " Updated workflows from Collibra API version 1 to Collibra API version 2\n",
            "and\n",
            " Migrated all the community structure, Assignments, Metadata, Workflows and Views to other\n",
            "Governance\n",
            "controlled environments\n",
            "workflows\n",
            " Created user persona specific training content for understanding Collibra functionalities and\n",
            "workflows usage\n",
            " Managing project through Agile Methodology using Jira\n",
            " Understanding of SDLC and scrum methodologies\n",
            "Copyright © 2022, Xendat Data & Analytics\n",
            "Milad.izadian@xendat.com\n",
            "P age 1 | 2Responsibilities: -\n",
            " Creation of client specific Meta Model & Asset Model of existing excel based data on data\n",
            "governance platform\n",
            "Belgium’s Telecom\n",
            "Company  Understanding client’s processes and implementing Collibra workflows to enable data\n",
            "management capabilities\n",
            "Data Governance\n",
            " Creation of Data Governance Charter and Data Domain Playbook\n",
            "(Data Steward)\n",
            " Developed workflow for create/update new assets and domain\n",
            " Developed workflows for Asset lifecycle management process\n",
            "ACHIEVEMENT\n",
            "Skills Collibra, Groovy, BPMN flowable designer, Tableau, Microsoft office tools\n",
            " Collibra Ranger (Credential ID CCR.2020.082)\n",
            "Certifications  Collibra Expert Level 1, 2, 3 (Credential ID CCE03.20200102-1435) \n",
            "Infosys Agile Certification\n",
            "Copyright © 2022, Xendat Data & Analytics\n",
            "Milad.izadian@xendat.com\n",
            "P age 2 | 2\n",
            "Anand_Kishore_Resume_EA.docx\n",
            "\n",
            "\n",
            "3800 Powell Lane, #830, Falls Church, VA-22041\n",
            "Mobile: (202) 751-5083 │ anandkishore1@icloud.com \n",
            "LinkedIn: http://linkedin.com/in/anand-kishore-05351336\n",
            "PROFESSIONAL SUMMARY \t\n",
            "Collaborative technical leader in digital health analytics product development since 2008 with a passion for engineering new BI (EDW, Big data, Reporting & Dashboard, Analytics and Data science) solutions and growing revenue. Skilled at leading large complex initiatives to achieve strategic business objectives, while managing people, processes and technologies using SAFe best practices. Innovative, solution-focused professional leveraging expertise in creating/piloting the strategic vision, planning, disciplined execution, M&A integration facilitation to achieve business and product line goals and effective communication at all levels of an organization.\n",
            "\n",
            "EDUCATION  \t\n",
            "B. Tech in Electrical & Electronics Engineering, May 2008\n",
            "Dr. MGR Educational and Research Institute, Chennai, India\n",
            "\n",
            "EXPERIENCE   \t\n",
            "Technical Delivery Lead (Enterprise Data Platform)                        \tMay 2020– Till Date\n",
            "KMM Technologies\n",
            "Client Name- Carefirst BCBS (Reston, VA)\n",
            "\n",
            "As Technical Delivery Lead, reporting to the Director, Enterprise Data Platform and leading the company’s Data strategy and BI solutions and implementations, such as Cloud Migration, EDW, Big data, Data Management, Analytics, reporting that are required for day-to-day operations, such as Claim (medical and Rx) processing, new enrollments & renewals, Benefit Administration, Care management etc. \n",
            "Lead the design, development and architecture of the FEPOC’s DWBI initiatives\n",
            "Define AWS cloud architecture and migration strategy for implementing a complete cloud based big data solution using glue, Redshift, EMR, S3 etc and migrating the data platform from on-prem to AWS cloud\n",
            "Partnered with AWS Architects to optimize the design for maximizing the cost saving\n",
            "Define the long-term vision and roadmap for data platforms as well as modernize data and infrastructure\n",
            "Designed the real-time and batch data ingestion architecture using Lambda approach\n",
            "Familiarity with AWS IaaS, PaaS and SaaS offerings.\n",
            "Lead the Design and Development of medium to complex ETL jobs using the Ab-Initio GDE for real-time as well as batch data load.\n",
            "Drive creation of ETL, data policies, processes, standards and data governance workflows.\n",
            "Data Integration using the formats JSON, CSV, 834, 837 etc.\n",
            "Big Data Development using the technologies Hive, Impala, Pig and Kafka using Cloudera cluster.\n",
            "Development of Data Model using Erwin tool\n",
            "Responsible for project management, viz. Project tracking and status reporting using Jira\n",
            "Used SAFe, Scrum and Kanban methodologies for the software delivery\n",
            "Responsible for communication, education, and assistance in support of operational roll-out of Enterprise Data Platform.\n",
            "Associate Director (Data and Analytics solutions),\tSeptember 2014 – April 2020\n",
            "Evolent Health (Arlington, VA)\n",
            "As Associate Director (Data and Analytics solutions), reporting to the Managing Director, Information Management and lead the company’s BI solutions, such as Data Governance, EDW, Analytics, Reporting, for Evolent’s Value-based and Payer services clients and internal data consumers.\n",
            "Lead design, development and delivery of Enterprise data warehouse (DW/SQL/ETL), Big data, and Analytics solutions in Microsoft Azure cloud to help analytics and reporting users with all required information about the patient. Also, EDW was designed and optimized to create 360-degree view of patients to help care managers provide proactive care in order to cut down on medical costs with the utmost quality, support the client with utilization management and practice related solutions too. Following are some key projects delivered,\n",
            "Architected solutions using MS Azure PaaS service such as SQL server, Azure Synapse etc. \n",
            "Defined on-prem to Azure migration strategy, design and implementation plans for moving the data platform to MS Azure\n",
            "Detailed understanding of Azure Database and Analytics offerings\n",
            "System Design and development for integration of payer data (SQL), legacy (SAS based) and EDW (SQL based) systems to reduce the SLA to load data from payer application to all the way to DW (ETL and DW design) using technologies like SSIS, SQL server, Unix.\n",
            "Master data management solution for enrollment, claims, lab, encounter etc. data using Informatica MDM tool.\n",
            "Set and execute the Data governance strategy for the organization on Data Quality Framework (Informatica IDQ), Metadata Management (DAG Metacenter) and Reference Data Management (Orion Rhapsody).\n",
            "Implementation of Identity resolution/ Master patient index using Nextgate tool.\n",
            "Implementation of Big data solution (using Sqoop, Pig, Hive and MapR) to crunch and load high volume data like ENCOUNTER, LAB, CCD, EMR etc.\n",
            "Design data marts like ENROLLMENT, MEDICAL, Rx, ANCILLARY CLAIMS, ENCOUNTER, LAB and PROVIDER etc. using TOAD data modeler.\n",
            "Timely update of Reference data like Medispan, Optum, CMS, FDA for the data like NDC, ICD 10 diagnosis & procedure, HCPCS, CPT codes, Provider info, Rxnorm, Revenue codes, LOINC codes, SNOMED codes, etc.\n",
            "Designed and developed value add data like provider attribution, MEG, HCG and home-grown grouper technologies.\n",
            "Data integration using Orion Rhapsody tool for multiple formats for,\n",
            "Designing and developing new products/services related to health insurance exchange (HIX), CMS 5-Star, electronic health record (EHR/EMR), accountable care organizations (ACO), ICD-10 transition, care management, predictive modeling, business intelligence reporting - quality (HEDIS), risk adjustment (retrospective and prospective) and cost/utilization.\n",
            "Championed and evangelized adoption of agile product development and program management framework across the company.\n",
            "Led data management and delivery of HIPAA compliant / de-identified data in support of client’s needs.\n",
            "Developed Go-to-Market Strategy, including marketing / product collaterals and competitive analysis.\n",
            "Manage onshore and offshore shared-services teams (development and operations) located in Arlington, US and Pune, India, respectively.\n",
            "\n",
            "Technology Lead, EDW and Reporting, Infosys Ltd.                                                                     Feb. 2013 – Sep. 2014                   DC Health Exchange, Washington DC\n",
            "As Technology Lead reporting to the Delivery Manager and primarily responsible for the design, development and delivery of enterprise data warehouse & reporting solutions for DC Health Exchange, including cross-functional team management, strategic planning, budgeting, and liaison with DC management and stakeholders,\n",
            "Led the team size of more than 15 people across locations (onsite and multiple offshore locations and multiple teams) to design and develop enterprise data warehouse & reporting solutions for DC health insurance exchange.\n",
            "Worked closely with COTS vendors for converting Business Requirements into Technical Specifications.\n",
            "Followed Informatica Best Practices and Naming Conventions while developing the mappings, sessions and workflows.\n",
            "Developed EDW design, data model and Informatica complex mapping, workflow, reusable transformations and mentored team with the development work.\n",
            "Migration of the data from legacy ACEDS system to Health Exchange platform.\n",
            "Design and developed multiple medium/complex reports using SAP BO 4.1 version.\n",
            "Developed test strategies and infrastructure requirements for development using software like Informatica (for ETL) and Business objects (for reports development).\n",
            "Designed and led the development for the data quality framework using Informatica data quality suite.\n",
            "Managed quality aspect of project and experienced of executing the projects at CMM Level-5 (defect Analysis, Group reviews, Knowledge Management).\n",
            "Managed client relationships (C-suite and program leaders), including performance monitoring and reporting.\n",
            "Project Planning (using MPP).\n",
            "Making sure the quality aspect of project and experienced of executing the projects at CMM Level-5 (defect Analysis, Group reviews, Knowledge Management)\n",
            "Taking care of billing/invoicing, managing risks and working for mitigation plan and its execution in case risk materializes.\n",
            "Led strategic planning and portfolio reviews, including working with functional partners on data integration and governance processes, development initiatives and operational processes.\n",
            "\n",
            "\n",
            "Technology Analyst, Infosys Ltd.\tNov. 2011 – Jan. 2013 \n",
            "Healthways Inc., Tennessee, USA\n",
            "As Technology Analyst, reported to the Project Manager of Healthways. Primarily responsible for the requirement, analysis and development of EDW and Reporting solutions.\n",
            "Analyzing requirements from Client and understanding the current system.\n",
            "Designing and developing complex Informatica mapping and workflow.\n",
            "Development of Business Object Webi reports.\n",
            "Preparing Unit test cases.\n",
            "Planning and tacking the execution of the project.\n",
            "Regular interaction with client for status updates.\n",
            "Monitoring and recovery of the Informatica Jobs.\n",
            "Peer Code reviews.\n",
            "Performing Code Deployment activities to QA.\n",
            "In addition to EDW and Reporting solutions, led the development effort of data quality framework using Informatica data quality tool that was a value addition to the client.\n",
            "\n",
            "Senior System Engineer\tMay. 2009 – Sep. 2011\n",
            "Royal Bank of Scotland, UK\n",
            "As part of RBS Telephony Transformation Program, built an integrated Data warehouse solution for Telephony services for the analysis of the call events, agent events, different transaction services, customer request, termination reasons etc. and developed a semantic layer to provide a reporting capability to the client.\n",
            "\n",
            "Conceptualized, designed and managed business process automation to improve service level agreement and ease-of-use.\n",
            "Analyzed the existing system and done modifications to implement new logic.\n",
            "Coordinated with source system users, and monitored ETL process on a daily basis.\n",
            "Involved in creating sessions, configuring and testing sessions, workflows and various tasks using Informatica workflow manager.\n",
            "Mapplets and Reusable Transformations were used to prevent redundancy of transformation usage and maintainability.\n",
            "Created Mapping Parameters, Session parameters, Mapping Variables and Session Variables.\n",
            "Extensively used Informatica Power Exchange data maps for pulling Changed Data Capture (CDC) from Oracle 10g tables.\n",
            "Applied performance tuning on targets, sources, mappings and sessions.\n",
            "Involved in Performance Tuning of the Mappings, SQL statements, and used Explain Plan utilities for Optimum Performance.\n",
            "Coordinates application builds and release processes with QA Manager\n",
            "Data was extracted from spreadsheets and then loaded into Oracle tables using SQL Loader.\n",
            "Developed SQL scripts using procedures, functions and packages for processing business logic.\n",
            "Ran the SQL scripts through UNIX shell scripts.\n",
            "Supported production systems as required by optimizing performance, resolving production problems, and providing timely follow-up on problem reports.\n",
            "\n",
            "PROFESSIONAL DEVELOPMENT  \t\n",
            "\n",
            "\n",
            "TECHNOLOGY  \t\n",
            "\n",
            "\n",
            "Jennifer Morales (2021).docx\n",
            "Jennifer Morales\n",
            " \n",
            "8614 TIMBERBRIAR STREET, SAN ANTONIO, TX 78250 | C: 210-478-6971 | jenmor9382@yahoo.com\n",
            "Summary\n",
            "Investigative data analyst with 10+ years of experience working with businesses to define and standardize data.  Eager to collaborate in the development of new business solutions for Master Data Management.\n",
            "Experience\n",
            "Master Data Management Data Analyst/Enterprise Data Steward \t 03/2020 to present \n",
            "Multicare Health System \t Tacoma, WA \n",
            "Act as enterprise data steward\n",
            "Develop business requirements for MDM development\n",
            "Work with business/technical to gather, validate and confirm data associated with client/consumer requirements\n",
            "Seek out quality issues, work with business lines to resolve, and track data quality tickets for the team\n",
            "Maintain and update data dictionary\n",
            "Serve as subject matter expert on data migration \n",
            "Lead business discovery calls\n",
            "Migration mapping for data fields\n",
            "Lead code conversions\n",
            "Conducted testing for downstream systems, developed test scripts and test plans\n",
            "Experience working with local and vendor IT simultaneously to resolve environment and/or mapping defects\n",
            "Conduct user training for the system and data remediation\n",
            "Develop reference guides and training materials for standardized data management across all enterprise users \n",
            "Implement additional modules for credentialing software system to include configuration, testing, and implementation\n",
            "Develop data quality standards for master tables and lead governance efforts to steward\n",
            "Establish automations to reduce turnaround times, improve operation workflows, create consistency in data standards\n",
            "\n",
            "Network Development Specialist \t 12/2019 to 03/2020 \n",
            "Versant Health \t San Antonio, TX \n",
            "Coordinated and evaluated healthcare provider contracts\n",
            "Developed and managed provider relationships and contracting efforts\n",
            "Prepared analysis of contract details and statistics\n",
            "Distributed contract information and status updates within the organization\n",
            "\n",
            "Medical Staff Consultant III \t 12/2013 to 08/2019 \n",
            "The Greeley Credentialing Consulting Company \t San Antonio, TX \n",
            "Optimized processes by training employees to apply industry best data management practices\n",
            "Developed process improvements and streamlined procedures to effectively resolve stewardship discrepancies\n",
            "Supported executive decision-making by reporting on metrics and recommending actionable data quality improvements\n",
            "Strengthened traceability/lineage documentation by developing organization systems for records, reports and agendas\n",
            "Utilized well-developed active listening, interpersonal and communication skills in collaborating and requirements gathering with diverse individuals each day\n",
            "Led managed care auditing process and conducted internal audits\n",
            "\n",
            "Credentialing Application Analyst/Project Manager\t 06/2009 to 12/2013 \n",
            "Baptist Health System \t San Antonio, TX \n",
            "Create reports and provide database statistics and information, validated data to ensure system data integrity and monitored data security and privacy standards according to data use and rights agreements\n",
            "Served as project manager for new systems and enhancements to include:\n",
            "Led pre-migration mapping efforts for data fields by working directly with the vendor to identify and then correctly map all data fields from one version of the credentialing software to the other. Has the ability to lead users through conversations related to mappings. Also mapped data to state application to pre-populate at reappointment. \n",
            "Worked directly with vendor to create project timeline, identify technical team, and managed status updates\n",
            "Post migration activities included development and formatting of enterprise forms and user/leadership training \n",
            "\n",
            "Skills\n",
            "\n",
            "Education and Training\n",
            "Associate of Arts, In Progress, San Antonio, Texas \n",
            "\n",
            "High School Diploma, May 1998, San Antonio, Texas\n",
            "\n",
            "Profisee Master Data, October 2020, Tacoma, Washington\n",
            "\n",
            "Physician Leadership Institute, March 2019, San Antonio, Texas\n",
            "\n",
            "Risk Management: Avoiding Legal Pitfalls in Credentialing & Understanding Risk Management, February 2016, San Antonio, Texas\n",
            "\n",
            "\n",
            "bioinfo.pdf\n",
            "Peter Wu\n",
            "peterwu19881230@tamu.edu | (626) 261-9305\n",
            "PROFESSIONAL SUMMARY\n",
            "Results-driven Bioinformatics PhD | CS Master student with demonstrated Data Science proficiency consistently successful in\n",
            "leading data driven research, building high quality NGS software that leads to quality business decisions\n",
            "Problem Solver | Quick Learner | Passionate Team worker\n",
            "with: Python, R, SQL, HPC, AWS, Samtools, BWA | Many others\n",
            "RELATED WORK EXPERIENCE\n",
            "Computer Science, Texas A&M University College Station, TX\n",
            "Bioinformatics student researcher Sep 2021 –\n",
            "• Developing new machine learning / deep learning methods to classify microbiome genera from NGS data, aiming to identify\n",
            "pathogenic and influential microbes.\n",
            "ACT Genomics Taipei, Taiwan\n",
            "Bioinformatics & AI research intern May 2021 – August 2021\n",
            "• Co-pioneered a novel NGS software pipeline for fusion genes that speeds up 300% to the previous version, implemented with\n",
            "Python (eg. Pysam), Bash and Docker.\n",
            "• Involved in the statistical analyses of a cfDNA panel using R.\n",
            "Biochemistry & Biophysics, Texas A&M University College Station, TX\n",
            "Bioinformatics graduate research assistant August 2016 – April 2021\n",
            "• Led independent research projects in computational genomics, by systematically curating high-throughput phenotypes and\n",
            "apply statistical methodologies such as Mann–Whitney U test, Multiple Correspondence analysis and precision-recall, resulting\n",
            "in 4 SCI papers of which the main programming languages are Python and R.\n",
            "• Developed multi-omics statistical analysis pipelines using microbial phenotypes and genotypes.\n",
            "• Maintained MediaWiki-based website hosting high-throughput E. coli phenotype data\n",
            "• Supported other labs with needs to analyze RNAseq data.\n",
            "Biosciences, Rice University Houston, TX\n",
            "Molecular biology research intern August 2015– April 2016\n",
            "• Cultured mutant worms, maintain knock-out libraries and perform RNA silencing assay on C. elegans mutants in studying\n",
            "genetic interactions (eg. her1 gene for sex ratio studies), aiming at discovering gene regulatory network and finding treatment\n",
            "for Cancers.\n",
            "OTHER PROJECTS\n",
            "Machine Learning\n",
            "• Characterized microbial phenotypes by Random Forest, SVM and CNN\n",
            "• Built an XGboost combined with CNN model to help child learning (2019 Data Science Bowl Competition on Kaggle)\n",
            "• Built LASSO models to classify patients of Schizophrenia, Bipolar Disorder and Alzheimer based on RNA expression Data,\n",
            "which achieved >99% accuracy\n",
            "• Built a LASSO model to classify rice species that achieves >99% accuracy\n",
            "Cloud Computing\n",
            "• Developed website for computer engineering student association using AWS Cloud9 (as the Scrum Master)\n",
            "EDUCATION\n",
            "Texas A&M University, Department of Computer Science College Station, TX\n",
            "Master, Computer Science August 2020 - August 2022\n",
            "Texas A&M University, Department of Biochemistry & Biophysics College Station, TX\n",
            "PhD, Bioinformatics August 2016 - August 2021\n",
            "Journal publications\n",
            "Google Scholar Link\n",
            "LEADERSHIP & HONORS\n",
            "President of TT Biotechnology Association. December 2017 – December 2018\n",
            "• Organized an annual symposium of more than 100 people who came from Rice University, UT Austin, UT Southwestern, Baylor\n",
            "College of Medicine and Texas A&M. (https://www.ttbatw.org)\n",
            "Heep fellowship ($28,000) August 2018 – August 2019\n",
            "Rahul_Parihar_BI_Consultant_Resume.docx\n",
            "\n",
            "Rahul Singh Parihar\n",
            " Senior Data Consultant\n",
            "\n",
            "Rahulpar9@gmail.com\n",
            "\n",
            "\n",
            "Profile\n",
            "14+ Years Of overall IT experience\n",
            "8 Years of Data Warehousing ETL Informatica Experience\n",
            "6 Years of BI experience\n",
            "4 Years of Data Modeling Experience\n",
            "\n",
            "Summary\n",
            "\n",
            "IT: 13+ years of IT experience in Software Analysis, Design, Development, Implementation of business applications using waterfall and Agile methodology, aware of SOA standards and functionality. Done Projects for Health care, Pharmaceutical, Financial, Manufacturing and Automobile Verticals.\n",
            "Data Warehousing: Seven (8) years of ETL and data integration experience in Requirement gathering, Data Architecture, data mart designing and developing ETL mappings and scripts using Informatica PowerCenter 9.1 /8.6/8.1/8.0 and Power mart 4.7 using Designer, Repository Manager, Repository Server, Workflow Manager & Workflow Monitor, power Exchange, PowerConnect, Power connect for SAP, Star Schema, Snowflake Schema, OLTP, SQL*Plus, SQL Loader.\n",
            "Data Modeling: Four (4) years of Data Modeling experience using Erwin 8.x, [Dimensional Data modeling, Star Schema, Snow-Flake Schema, FACT and Dimensions tables, Physical and logical data modeling \n",
            "Business Intelligence: Three (6) years of Business Intelligence tools like IBM Cognos 7.x/8.x, Microsoft SSRS, SSAS, Web Intelligence 2.5, Retrieving data using Universes, Personal data files, Stored Procedures and Freehand SQL methods.\n",
            "Expertise at Enterprise Database Integration and Management among MS SQL Server, Oracle, Sybase, MS Access, SAP, Teradata and DB2.\n",
            "Databases: 13 years of experience using Oracle 10g/9i/8i/7.x, MS SQL Server 2008/2005, Teradata 12.0/V2R5/V2R4, DB2 9.1/8.0/7.0, IMS Data, SQL, PL/SQL, SQL*Plus, IBM DB2  Sun Solaris 2.x, HP-UX, Rational Clear Case, Rational Clear Quest, IBM Rational Team Concert Change Manager Tool.\n",
            "\n",
            "Education & Certification\n",
            "\n",
            "ECMP From University of Virginia \n",
            "Bachelors of Engineering in Computer Science\n",
            "\n",
            "Technical Skills:\n",
            "\n",
            "Professional Experience\n",
            "\n",
            "Deloitte Consulting Indianapolis Indiana, Aug 2017 – Present/ Sr. Data Modeler/BI Designer\n",
            "\n",
            "IEDSS (Indiana Eligibility Determination Service System) supports all Public Assistance programs (SNAP/TANF), Medicaid (ME) financial support to families, food assistance, childcare and other benefits. The project is to Collect the data from IEDSS worker portal, get it replicated through Oracle golden gate , apply the business logic through Informatica and load the data in Summary and Details tables for Cognos reporting and Key Performance Indicatory(KPI)evaluation. \n",
            "\n",
            "Responsibilities: \n",
            "\n",
            "Reporting data model Design\n",
            "Informatica Mapping and Workflow Design \n",
            "Parameter File Design Changes \n",
            "Dynamic Parameter file Design\n",
            "Review of Informatica Mapping and Workflow where Source Qualifier, Joiner, Union, Aggregator, sorter, Expression, transaction control, XML transformation and lookups transformations are used.\n",
            "Design recommendation for Performance enhancement of Informatica mapping using advance techniques using dynamic caching, memory management and  parallel Processing \n",
            "Designing detailed Program specification \n",
            "Actively taking part in change management and validation of the changes.\n",
            "Cognos normal and complex dashboard report review\n",
            "Validation of near real time Cognos report.\n",
            "\n",
            "Environment: Oracle 10g, Informatica PC 10.1, CA7, IBM Cognos 8.4 Report Net (Framework Manager, Report Studio, Query Studio), Linux.\n",
            "\n",
            "\n",
            "Independent Health Williamsville New York, Sep 2016 – July 2017/ Data Modeler/ Data Analyst \n",
            "\n",
            "Independent Health is a leading health insurance provider in USA, having head office in Williamsville NY. The project objective was to bring down the inventory of BI production tickets, reduce the count of aging tickets, Complete Root cause analysis of issues and applying fixes accordingly and reduce the inflow of incidents. Project also include proof of concept and code enhancements. \n",
            " \n",
            "Responsibilities: \n",
            "\n",
            "Proactive analysis of tickets to detect repeatable patterns \n",
            "Logical and Physical Data Modeling of Reporting Schema.\n",
            "Design and peer review of Informatica workflow improvement \n",
            "Improvement of Tidal timings and threshold by improving the core Informatica job performance\n",
            "Code Review \n",
            "Monitoring of Data Quality and providing recommendations for internalizing improvement as part of overall implementation activities.\n",
            "Design and Development of checklists to encourage development best practices\n",
            "Designing the error handling, error reporting and reprocessing framework.\n",
            "Defining the IH development standard for the team. \n",
            "Working closely with the Enterprise architectures team and development team to resolve production issues root cause analysis and enhancements. \n",
            "Executed queries using Hive and developed Map-Reduce jobs to analyze data. \n",
            "Developed the Pig UDF's to preprocess the data for analysis. \n",
            "Developed Hive queries for the analysts.\n",
            "Utilized Apache Hadoop environment by Hortonworks. \n",
            "Involved in designing the data loading strategy from LINUX and UNIX file system to HDFS. \n",
            "Designing of Importing and exporting data in HDFS and Hive using Sqoop.\n",
            "\n",
            "Environment: Apache Hadoop (Horton works), HDFS, Pig, Hive, Sqoop, Map Reduce, SQL Server 2012, Oracle 10g , Informatica PC 10.1, Cisco Tidal , Db2, Linux .\n",
            "\n",
            "\n",
            "Humana Inc. Louisville Kentucky Sep 2015 – Sep 2016 ETL Architect/Data Modeler \n",
            "\n",
            "Human Inc. is a leading health insurance provider in USA, having head office in Louisville KY. The project is to collect and process the incoming data from various vendors in HIPPA compliance format and use it further for analytic processing for Population Healthcare Management purposes (PHM).\n",
            "\n",
            "Responsibilities: \n",
            "\n",
            "Designing and Proposing the Architecture of the new system \n",
            "Designing the architecture to process, the HIPPA compliance files like HL7, 837, CCD etc.\n",
            "Designing the error handling, error reporting and reprocessing framework.\n",
            "Design and development of Incoming proprietary format files and performing analytics operation on them.\n",
            "Defining the development standard for the team. \n",
            "XML Schema development.\n",
            "Maintaining Patient Metadata.\n",
            "Modeling the data storage layer.\n",
            "Working closely with the Enterprise architectures and development team to build the end-to-end solution using agile methodology.\n",
            "\n",
            "Environment: SQL Server 2012, Oracle 10g, Informatica PC 9.6, Data Exchange B2B libraries, $Universe, XML Spy, MS Visio.\n",
            "\n",
            "\n",
            "StateStreet Corporation Boston MA Dec 14 –Sep 15 Data Modeler/Application Architect\n",
            "\n",
            " StateStreet is a US-based international financial services holding company. State Street was founded in 1792 and is the second oldest financial institution in the United States. EC (Enhance Custody) Consolidation is an IT initiative to maintain an Enterprise Wide Data warehouse to cater various department reporting needs.\n",
            "\n",
            "Responsibilities: \n",
            "Designing and Proposing the Architecture of the new system \n",
            "Financial Risk Data Mart Designing \n",
            "Modeling the Data Mart for reporting\n",
            "Designing Database Migration activities coordination from Oracle 10g to Oracle Exadata\n",
            "Working Closely with the Development team and DBA’s for building the Data warehouse.\n",
            "\n",
            "Environment: Erwin 9.1, DataStage, BM Cognos 8.4 ReportNet (Framework Manager, Report Studio, Query Studio), Oracle Exadata , PL/SQL, Flat Files, MS Visio.\n",
            "\n",
            "\n",
            "Daimler Trucks North America (DTNA) A Mercedes Company, Portland, OR     Sep’12 – November 14, Data Modeler\n",
            "\n",
            "DTNA is a leading Truck manufacturer having head office in Portland Oregon. NPS deals with DND group and rewrite of the current Dealer operation Database and other external systems. This Project caters remodeling the database and changes to make the infrastructure to be compatible with Windows 7.\n",
            "\n",
            "Responsibilities:\n",
            "Working on the integration of existing systems at Data warehouse and Application systems level.\n",
            "Worked to standardizing the new architecture, data flow and documentation of the process and business flow at enterprise level.\n",
            "Working on identifying the applications as systems of records between the two companies and designing the new enhanced integrated warehouse for reporting needs.\n",
            "Participated in all phases of project including Architecture, Design discussion, Requirement gathering, Analysis, Coding, Testing, Documentation and warranty period.\n",
            "Extensively used SQL for Data Analysis and to understand and documenting the data behavior within and between two companies.\n",
            "Reverse engineered existing databases to understand the data flow and business flows of existing systems and to integrate the new requirements to future enhanced and integrated system.\n",
            "Designing the procedures for getting the data from all systems to Data Warehousing system. The data was standardized to store various Business Units in tables. \n",
            "Working with ETL Architects and developer to design performance centric etl mappings\n",
            "Worked on defining deadlines and helped team to meet the deadlines.\n",
            "Extensively worked on documentation of Data Model, Mapping, Transformations and Scheduling jobs.\n",
            "Working extensively with Business Objects Report developers in creating data marts and develop reports to cater the existing business needs.\n",
            "Designed Mapping Documents and Mapping Templates for SSIS and Informatica ETL developers.\n",
            "Deployed naming standard to the Data Models at enterprise level and followed company standard for Project Documentation.\n",
            "Designing ER diagrams, logical model (relationship, cardinality, attributes, and, candidate keys) and convert them to physical data model including capacity planning, object creation and aggregation strategies, partition strategies, Purging strategies as per new architecture.\n",
            "Working on Gap Analysis of existing warehouse models.\n",
            "Data analysis of existing data base to understand the data flow and business rule applied to different data bases by SQL\n",
            "Conducted meetings with the business and technical team to gather necessary analytical data requirements in JAD sessions.\n",
            "Partitioned tables based on the requirement and developed purging and archival rules.\n",
            "Designing and developing strategies for Data Conversions and Data Cleansing\n",
            "Creating Data mappings, Tech Design, loading strategies for ETL to load newly created or existing tables.\n",
            "Created Schema objects like Indexes, Views, and Sequences, triggers, grants, roles, Snapshots.\n",
            "Pre populate the static tables in the Data warehouse.\n",
            "Developed strategies and loading techniques for better loading and faster query performance.\n",
            "\n",
            "Environment: Erwin 8.2, Informatica PowerCenter 9.1, SSIS, IBM Cognos 8.4 ReportNet (Framework Manager, Report Studio, Query Studio), Oracle 11g, DB2, MS SSRS/SSAS PL/SQL, T-SQL, Flat Files, MS Visio, Informatica Power Center 9.1.\n",
            "\n",
            "Seagate Technology LLC, Scott’s Valley, CA, USA\t                                           Nov‘11–Sept’12\n",
            "Business Intelligence consultant \n",
            "\n",
            "Description: Seagate Technologies is a leading hard drive manufacturer having head office in Scott’s valley, California. EDW HRBI deals with the HR data of the company; dashboard reporting enables the higher management to take timely and effective HR decisions. This Project caters up gradation of Informatica from 8.6 to 9.1, post upgrade testing for Functional and technical aspects followed by support to the application.\n",
            "\n",
            "Responsibilities:\n",
            "Involved in gathering technical requirements and written technical specifications.\n",
            "Developed complex mappings in Informatica to load the data from various sources using different transformations like Source Qualifier, Look up (connected and unconnected), Expression, Aggregate, Update Strategy, Sequence Generator, Joiner, Filter, Normalizer, Rank and Router transformations.\n",
            "Worked on Informatica tool –Source Analyzer, Data warehouse designer, Mapping Designer & Mapplets and Transformations.\n",
            "Worked on an integration project where data from the newly acquired company (3Com) data is loaded into HP data warehouse.\n",
            "Involved in the development of Informatica mappings and tuned for better performance.\n",
            "Extensively used ETL to load data from flat files, Oracle-to-Oracle database and Flat Files.\n",
            "Written Perl and UNIX scripts for data cleansing and loading data into Oracle tables.\n",
            "Worked on redesigning of one of the projects to automate the system and improve consistency and performance\n",
            "Involved in the Unit testing, Event and System testing of the individual.\n",
            "Analyzed existing system and developed business documentation on changes required. \n",
            "Used UNIX to create Parameter files and for real time applications.\n",
            "Created UNIX shell scripts to validate the source files and to cleanse the data and this final file is used as one of the sources in Informatica.\n",
            "Extensively involved in testing the system from beginning to end to ensure the quality of the adjustments made to oblige the source system up-gradation. \n",
            "Efficient Documentation was generated for all mappings and workflows.\n",
            "Efficient Unit testing documentation was created along with Unit test cases for the development code.\n",
            "Detail Technical design documents were prepared before the code was migrated.\n",
            "Detailed System Defects were created to inform the project team about the status throughout the process.\n",
            "\n",
            "Environment: Informatica PowerCenter9.1 (Source Analyzer, warehouse designer, Perl 5.6.1, Mapping Designer, Mapplet Designer and Transformations), Cognos 8.2, Oracle 10g, Toad 10.5,Windows Vista, HP-UX, Reflection X,  Rational Clear Case, Rational Clear Quest\n",
            "\n",
            "AGL Resources, Atlanta, GA, USA\t                                \t                              June ’11 – Nov’11\n",
            "Informatica ETL consultant \n",
            "\n",
            "AGL Resources (Atlanta Gas and Light) is a leading Natural Gas distribution agency in USA. The Project is \n",
            "To support the Customer Management Application (CMA) for its retail and commercial customers billing and scheduling needs. The CMA application is being implemented using Informatica Power Center and Power Exchange on Sql Server Database. It takes data from Legacy Mainframe System and record and process changes in the CMA system through Informatica Power Exchange.\n",
            "\n",
            "Responsibilities: \n",
            "Worked on Informatica tool –Source Analyzer, Data warehouse designer, Mapping Designer & Mapplet and Transformations.\n",
            "Extensively used ETL to load data from flat files, DB2 to SQL Server database and Flat Files.\n",
            "Extracted data from XML sources and compared the data with Historical data available in the warehouse.\n",
            "Created data quality plans for address validation and for data validation purposes\n",
            "Used the data quality plans to determine the quality of the customer data on the portal when compared to the data available in the source systems.\n",
            "Used Informatica data explorer tool to generate the reports on profiled data.\n",
            "Validated for the accuracy of the customer, product information that is presented on the portal (web interface for the customer to see his/her insurance details).\n",
            "Involved in Team meetings to set up best practices guide and sop’s.\n",
            "Worked with the technical lead and the developers to create templates for code migration and code review documents.\n",
            "Involved in the Unit testing, Event and System testing of the individual.\n",
            "Analyzed existing system and developed business documentation on changes required. \n",
            "Documentation was generated for all mappings and workflows before the code migrated to QA.\n",
            "Wrote Perl scripts to handle the data cleansing issues related to the data. \n",
            "Efficient Unit testing documentation was created along with Unit test cases for the development code.\n",
            "Detail design document was created before the code walk thru meetings with the team.\n",
            "Detail Technical design documents were prepared before the code was migrated.\n",
            "Created CR’s (Change Request) when the code is ready for migration to different environments.\n",
            "Given support for the production team for the first 3 weeks after the code migrated to production environment.\n",
            "\n",
            "Johnson & Johnson, Piscataway, NJ, USA                                                               Jan’11-June’11\n",
            "Data Warehousing / ETL Developer\t\t                     \t\t  \n",
            "\n",
            "The main objective of Crossroad the data conversion project is to give JNJ management a common platform for reporting the business object data for all the child and acquired companies in cleansed and unique manner. The Data Warehouse implemented for this cause take the input from various sources in the form of Flat files, cleanse and profile the data and finally load it into SAP system using Informatica BAPI adapter. Cross road project was to deliver more value to J&J management team and users, while reducing the overall cost and complexities.\n",
            "\n",
            "Responsibilities:\n",
            "Involved in requirement gathering technical requirements \n",
            "tuned existing Informatica maps for better performance \n",
            "Worked with various transformations like Source Qualifier, Look up (connected and unconnected), Expression, Aggregate, Update Strategy, Sequence Generator, Joiner, Filter, Rank and Router transformations.\n",
            "Involved in massive data cleansing and data profiling of the production data load.\n",
            "Created synonyms for copies of time dimensions, used the sequence generator transformation type to create sequences for generalized dimension keys, stored procedure transformation type for encoding and decoding functions and Lookup transformation to identify slowly changing dimensions.\n",
            "Created FTP scripts to transfer data from consumer systems to HCA systems, and Conversion scripts to convert data into flat files to be used for Informatica sessions. \n",
            "Extensively involved in the Analysis, Design and Modeling various levels of the system. Worked with Erwin and MS Visio for the Data modeling and designing various other technical specifications.\n",
            "Worked on Informatica tool –Source Analyzer, Data warehousing designer, Mapping Designer & Mapplet and Transformations.\n",
            "Involved in the development of Informatica mappings and tuned for better performance.\n",
            "The four stages – from Flat Files up to the Warehouse\n",
            "Extensively used ETL to load data from flat files (excel/access) to Teradata database.\n",
            "Load balancing of ETL processes, database performance tuning and Capacity monitoring.\n",
            "Involved in the Unit testing, Event & Thread testing and System testing of the individual.\n",
            "Analyzed existing system and developed business documentation on changes required. \n",
            "Used UNIX to run FTP scripts, which transferred data from the UNIX server to the Windows NT server. \n",
            "Made adjustments in Data Model and SQL scripts to create and alter tables.\n",
            "Worked with many existing Informatica Mappings to produce correct output. \n",
            "Made use of Server Manager to create and monitor sessions and batches. \n",
            "Efficient Documentation was generated for all mappings and workflows.\n",
            "\n",
            "Johnson & Johnson, Titusville, NJ,                                                                          Mar’10 – Jan’11                              \n",
            "Data Modeler and Designer \n",
            "\n",
            "Johnson and Johnson is a world renowned pharmaceutical company that discover, develops, manufactures and markets a broad range of drugs and medicinal products through its own plat and through acquired companies. The Scope of this project was to build an Enterprise Wide Data Warehouse to accommodate sales data from various markets and to use them for reporting, analysis in deciding the compensation for Medical representatives. This Data Warehouse was implemented using multiple flat files as source using oracle database as target and Maestro as scheduler.  \n",
            "\n",
            "Responsibilities:\n",
            "Extensive interaction with users to understand their requirements, coordination with other project teams to arrive at synergy and unified development plan.\n",
            "Developed sessions using Workflow Manager for loading data into target Database.\n",
            "Worked on Performance Tuning of various mappings and sessions to identify and remove Bottle Necks.\n",
            "Designing of Error Handling Strategies. \n",
            "Supported users with word processing, spreadsheets, databases and presentation software.\n",
            "Data modeling and design of data warehouse and data marts in Snowflake and Star schema methodology with confirmed and granular Dimensions and FACT tables\n",
            "Designing of Physical and Logical Data Model using the Erwin tool.\n",
            "Maintaining the Metadata of the Data Model \n",
            "Keeping track of the changes and various version of Data Model intact.\n",
            "Proposing solutions to the Design Challenges.\n",
            "Project Plan and Resource Estimation.\n",
            "Writing UAT test cases and UAT Support.\n",
            "Involved in analyzing database schema using Erwin\n",
            "\n",
            "Environment: Informatica Power Center 9.1, Oracle 10g, Cognos8.2 , MS Access, Excel, Toad 7.3, Erwin 4.0, Unix.\n",
            "\n",
            "Novartis, Parsippany, NJ,                                                                                          Oct’09 – Jan’10         \n",
            "Oracle Database Consultant\n",
            "\n",
            "Analysis, Loading and extracting of data for MCM campaign, which is targeted for various physicians, to convey information about the Novartis Products through multiple channels (Phone, mail, IVR, SMS, Web etc.).\n",
            "\n",
            "Responsibilities:\n",
            "Analyzing the source data coming from different sources and working with business users and developers to develop the Model. \n",
            "Involved in Dimensional modeling to Design and develop STAR Schema, Using ER-win to design Fact and Dimension Tables. \n",
            "Extracted, Transformed and Loaded OLTP data into the Staging area and Data Warehouse using Informatica mappings and complex transformations (Aggregator, Joiner, Lookup (Connected & Unconnected), Source Qualifier, Filter, Update Strategy, Stored Procedure, Router, and Expression). \n",
            "Developed number of complex Informatica Mappings, Mapplets and Reusable Transformations for different types of tests in Customer information, Monthly and Yearly Loading of Data. \n",
            "Using Workflow Manager for Workflow and Session Management, database connection management and Scheduling of jobs to be run in the batch process. \n",
            "Fixing invalid Mappings, testing of Stored Procedures and Functions, Unit and Integration Testing of Informatica Sessions, Batches and Target Data. \n",
            "Extracted data from various sources like IMS Data Flat Files and Oracle.  \n",
            "Extensively Used Environment SQL commands in workflows prior to extracting the data in the ETL tool. \n",
            "Wrote complex SQL scripts to avoid Informatica joiners and Look-ups to improve the performance, as the volume of the data was heavy.\n",
            "Created Sessions, reusable Worklets and Batches in Workflow Manager and Scheduled the batches and sessions at specified frequency. \n",
            "Used SQL * Loader for Bulk loading.\n",
            "Wrote UNIX Shell Scripting for Informatica Pre-Session, Post-Session Scripts.\n",
            "Created Stored Procedures for data transformation purpose. \n",
            "Monitored the sessions using Workflow Monitor. \n",
            "Used stored procedures to create a standard Time dimension, drop and create indexes before and after loading data into the targets.\n",
            "Removed bottlenecks at source level, transformation level, and target level for the optimum usage of sources, transformations and target loads.\n",
            "Captured data error records corrected and loaded into target system.\n",
            "Created Mappings, Mapplets and Transformations, which remove any duplicate records in source.\n",
            "Implemented efficient and effective performance tuning procedures, Performed benchmarking, and these sessions were used to set a baseline to measure improvements against.\n",
            "Tuned Source System and Target System based on performance details, when source and target were optimized, sessions were run again to determine the impact of changes.\n",
            "Created reports using Business Objects, Universes, Stored Procedures and Free Hand SQL as the Data Providers. \n",
            "Used Calculations, Variables, Break points, Drill down, Slice and Dice and Alerts for creating Business Objects reports. \n",
            "\n",
            "Environment: Informatica Power Center 6.2, Oracle 9i, Business Objects 6.0, MS SQL Server 2005, DB2, IMS Rx Data, Oracle 10g, SQL, PL/SQL, Erwin 4x, AIX, Unix Shell Scripts, Toad\n",
            "\n",
            "GSK (Glaxo Smith Kline), London, UK                                                                     Feb’09 - Sep’09      \n",
            "Data warehouse Developer / ETL Infromatica Consultant\n",
            "\n",
            "CIDR (Commercial Information Data Repository) was a project to replace the existing Informatica mapping with the Pl/Sql and UNIX script in order to save the Licensing cost of the Informatica tool.\n",
            "\n",
            "Responsibilities:\n",
            "Interacted with end-users and business analysts to identify and develop business requirements and transform it into technical requirements and ultimately responsible for delivering the solution\n",
            "Conversion of Infromatica Mapping to Oracle PL/SQL packages.\n",
            "Replacing and testing the call to the workflow in the scheduler with newly develop oracle packages. \n",
            "Involved in loading the data from legacy systems to Oracle \n",
            "Performance improvement.\n",
            "Writing Shell Scripts to schedule newly build Pl/SQL packages.\n",
            "Writing Unit Test cases.\n",
            "\n",
            "Environment: Informatica Power Center 8.6.1, Oracle10g, Sun Solaris 8, UNIX Shell Scripts\n",
            "\n",
            "Continental AG, Hanover, Germany                \t\t\t\t          Sep’08 - Dec’09\n",
            "Oracle Warehouse Consultant/Developer                                                                    \n",
            "\n",
            "Market Share Reporting (MSR) was an application to provide visibility into the Canadian operations. This helped Continental Tires to analyze their position in the Canadian Market, and make better decisions for Product Planning, Forecasting and other business operations.\n",
            "\n",
            "Responsibilities:\n",
            "Performed DBA and application analysis tasks.\n",
            "Developed a Web bases tool using Oracle Apex to Export and Import the data file form the mail box to the UNIX box and Vice versa.\n",
            "Created Mapping to load the data from Source to Target using OWB and Oracle as database. \t\n",
            "Created Database objects including Tables, Indexes, Clusters, sequences, roles, and privileges.\n",
            "Created and maintained Triggers, Packages, Functions and Procedures.\n",
            "Analysed database for performance issues and conducting detailed tuning activities for improvement.\n",
            "Wrote procedures to meet ad-hoc requirements to replicate data from one table to another.\n",
            "\n",
            "Environment: Oracle Pl/Sql, Oracle 10g, UNIX, Oracle Warehouse Builder (OWB), Oracle Apex\n",
            "\n",
            "Cisco, Bangalore, India             \t\t\t\t                                 Oct’07 - Aug’08\n",
            "Performance Tuning Expert                                                                    \n",
            "\n",
            "WWSPS-Performance. The Project deals with the performance tuning need of various sales applications running for Cisco and providing suggestions for performance improvement\n",
            "\n",
            "Responsibilities:\n",
            "Performed DBA and application analysis tasks.\n",
            "Analysed database for performance issues and conducting detailed tuning activities for improvement.\n",
            "Created separate table spaces for Users, Indexes, Rollback Segments and Temporary Segments. Renaming and resizing of redo log files, user data files, rollback files, index files and temporary files\n",
            "Wrote procedures to meet ad-hoc requirements to replicate data from one table to another.\n",
            "Giving performance improvement Suggestions to the application team by showing them the TKprof results and showing the CBO (cost-based optimizer) statistics after applying the recommendations.\n",
            "Giving Performance improving suggestions using hints, indexes, improving the table spaces, tuning the heavy Sql’s, rebuilding the tables and other performance improvement methods.\n",
            "\n",
            "Environment: Oracle Pl/Sql, Oracle 10g, UNIX.\n",
            "\n",
            "Xerox Corporation, Bangalore, India            \t\t\t                     Dec’06 - Sep’07\n",
            "Oracle Warehouse Consultant/Developer \n",
            "\n",
            "ISRVE reporting system is a web-based reporting interface tool mainly built for the managers, business unit leaders and field personals in Xerox to locate and interpret information at a level of detail that will allow managers to identify problems and make actionable \n",
            "\n",
            "Responsibilities:\n",
            "Performed DBA and application analysis tasks.\n",
            "Manual Testing, Onsite/Offshore coordination\n",
            "Assigning the task to the team members and coordination of deliverables.\n",
            "Test case Preparation\n",
            "\n",
            "Environment: Oracle Pl/Sql, Oracle 10g, UNIX, Oracle Warehouse Builder (OWB)\n",
            "\n",
            "SEAGATE, Bangalore, India              \t\t                      \t                                July ’06 - Nov’06\n",
            "Oracle Consultant/Developer \n",
            "\n",
            "The main objective for developing this interface is to update the order information back to the ecommerce system from return management system for resulting it to synchronizing of the order information between the B2C E commerce system and the Return Management System.\n",
            "\n",
            "Responsibilities:\n",
            "Writing Design Specification \n",
            "Manual Testing, Onsite/Offshore coordination\n",
            "Assigning the task to the team members and coordination of deliverables.\n",
            "Test case Preparation\n",
            "\n",
            "Environment: Oracle PL/SQL, XML, Windows 2000 Professional, UNIX\n",
            "\n",
            "I-Flex Solutions Ltd (Now Oracle Financial Services)                                              Jul’04 –Jul’06\n",
            "\n",
            "I-Flex Solutions is a world leader in providing IT solutions to the financial services industry, with more than 790 customers in over 130 countries. Its range of applications software which includes No1 core banking software Flex Cube, custom solutions and consulting services enable financial institutions to cut costs, respond rapidly to market needs, enhance customer service levels and mitigate risk.   \n",
            "\n",
            "KEY PROJECTS: \n",
            "Flex Cube Corporate (FCC) Enhancements                                                             July 04- July 06\n",
            "\n",
            "FS, DS Preparation, review meeting and giving proof of concept, efficiently manage coding, design of database, support to the Integration testing, Mentoring rookies.\n",
            "\n",
            "S No \tClient\tTitle\n",
            "1\tTA-CHONG Bank, Taiwan\tInterest basis definition at Schedule Level for LIBOR rate accrual Notes.\n",
            "2\tIndover bank\tLink the FX_REF_No in LC/BC/LD module.\n",
            "3\tNCB (National Cooperative Bank) Bank JIDDAH, Middle East\tMemo Contingent Entry for Customer Portfolio Settlement\n",
            "4\tSHAMIL BANK, UAE\tPrepayment of Deposit accounts, Inter branch processing for the fund transactions.\n",
            "\n",
            "5\tEFG SERBIA BANK\tProcessing the MT-998-796 messages\n",
            "6\tNCB JIDDHA BANK\tSingle Debit to Nostro for Customer Portfolio\n",
            "\n",
            "Environment: Oracle PL/SQL, Oracle D2K, Windows 2000 Professional, UNIX, Oracle D2K Forms and Report 6i\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "JoeNovella_Resume-Vertex.docx\n",
            "Joseph Novella\n",
            "Senior Data Quality Consultant\n",
            "635 Barrie Rd, Middletown, DE 19709\n",
            "Office: 201-805-3973\n",
            "novellaj@verizon.net \n",
            "\n",
            "\n",
            "Summary: \n",
            "A thirty-year veteran of the data quality and data management discipline, with experience in both technical and methodology implementations across multiple industries including pharmaceutical, manufacturing, and financial services.  He has acquired business and technical expertise in building customized parsing, standardization, and cleansing rules for many different types of data including name/address, financial, and product subject areas.\n",
            "\n",
            "\n",
            "CERTIFICATIONS\n",
            "SAS Certified Associate: Programming Fundamentals Using SAS 9.4\n",
            "\n",
            "TECHNICAL EXPERTISE\n",
            "SAS Data Management Platform (SAS data quality product)\n",
            "Evoke Axio data profiling product\n",
            "Similarity Systems data quality product\n",
            "Structured Query Language (SQL)\n",
            "Oracle\n",
            "SQL Server\n",
            "SAS Programming\n",
            "\n",
            "LEADERSHIP\n",
            "Data Quality Team Lead for a seven person team\n",
            "Data quality consulting practice lead for three person team \n",
            "Active oversight in technology and software implementations\n",
            "Accountability around project requirements & deliverables\n",
            "Customer satisfaction & consulting relationships\n",
            "Proven success in client engagements & solution designs\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "PROFESSIONAL EXPERIENCE\n",
            "SAS\t\t\t\t\t\t\t\t2006 – 2021\n",
            "Technical Architect and Data Quality Team Lead: 2020-2021\n",
            "Led a team of seven (7) focused on developing and delivering data quality services and solutions to clients.  Provided post-sales training, software implementation, design, and configuration services in the DataFlux™ Data Management.  Implemented data assessment, quality improvement, data governance and monitoring processes in support of master data management and data warehouse projects.  Scoped project requirements and provided corresponding project plans outlining work effort involved.  Developed strong working relationships with new and existing customers.  Designed and developed new country identification solution that utilized Natural Language Processing techniques to identify the country of origin from a street address. Researched machine learning techniques for data quality management\n",
            "\n",
            "Intelligence and Investigation Management: 2021\n",
            "Technical Lead in implementing the SAS Intelligence and Investigation Management (IIM) product for North Carolina Criminal Justice.  Directed two (2) team members in localization of IIM solution.  Translated requirements into technical specifications for the database design, user interface, security layer, and workflow design.  Scope included Requests for Information, Suspicious Activity Reports, and Intelligence Reporting modules\n",
            "\n",
            "Master Data Management \n",
            "Pharmaceutical (2007-2010): Implemented complex business and standardization rules within the SAS Quality Knowledge Base (QKB).  Designed and developed new parsing and standardization processes, utilizing targeted Natural Language Processing techniques, in support of Product Master Data Management projects.  Developed comprehensive, automated processes to consolidate and improve product data\n",
            "Anti-Money Laundering (2013-2021): Led entity resolution and data quality design and development efforts in support of AML projects for four international banks\n",
            "\n",
            "Data Quality / Data Migrations\n",
            "Insurance (2006-2008): Developed automated monitoring processes for existing data sources for a claims data warehouse environment.  Contributed to the design of the Data Governance process\n",
            "Pharmaceutical (2008-2010): Designed and developed entity resolution matching and cleansing processes for a customer relationship management (CRM) environment.  These processes included standardizing and verifying name, address, and phone number data.  Assisted in data quality improvement for Human Resource data in preparation for a migration into the SAP HR environment.  Designed a process that reports on the content and quality of existing data sources.  Developed the reports and dashboards needed for the Data Governance process\n",
            "Retail (2010-2012): Developed extraction and standardization processes for product-related data for a major retail client.  Custom processes were built to extract and standardize product characteristics using Natural Language Processing techniques, and automated processes to populate the target Oracle database were developed in support of the client’s online customer purchasing website.  Client increased online sales 50% year over year directly attributed to the project\n",
            "\n",
            "Conversion Services International, Inc.\t\t\t2003-2006\n",
            "Senior Data Quality Consultant\n",
            "Designed and managed data quality practice with three (3) team members in support of data warehousing “best practices” methodology.  Incorporated data profiling technology into data quality methodology.  Performed data quality, migration, and integration services for the following customers\n",
            "\n",
            "Automated Data Processing (ADP)\n",
            "Developed strategy for client file address matching and client demographic augmentation for existing data warehouse environment\n",
            "\n",
            "Pfizer Corporation\n",
            "Developed comprehensive data integration and migration strategy and plan for clinical trial subject area data warehouse.  Included data matching and cleansing rules and routines\n",
            "Assessed case tracking source data for integration into existing data warehouse environment.  Assisted in documenting data transformation rules and mappings, based on data profiling results\n",
            "\n",
            "Anthem Consulting, LLC\t\t\t\t\t2002-2003\n",
            "Principal Consultant\n",
            "Formed consulting firm delivering data profiling and data management consulting services for organizations implementing data warehouse, CRM, ERP, and data quality efforts.  These services include project planning, mentoring, training, methodology development, and seminars.  Developed the Information Accounting service for data assessment efforts\n",
            "\n",
            "Published “The Data Profiler Newsletter” (www.TheDataProfiler.com) web newsletter dedicated to the data profiling discipline and industry\n",
            "\n",
            "Performed data profiling and assessment services for the following companies\n",
            "\n",
            "Sentry Insurance\n",
            "Developed integrated model for a metadata repository combining source system, data warehouse, data profiling, and ETL information needs\n",
            "\n",
            "Kraft Foods\n",
            "Trained Kraft personnel in the Evoke Axio™ product line.  Developed mentoring process by incorporating actual projects into training process\n",
            "\n",
            "Evoke Software Corporation\t\t\t\t\t1999-2002\n",
            "Senior Consultant\n",
            "Conducted Pre-Sales and Post-Sales activities including Product Demonstrations, Proofs of Concept, Product Installations, Training, and Consulting assignments.  Represented Sales in product design sessions by developing enhancement requests based on new customer requirements\n",
            "Managed and conducted over thirty (30) Proofs of Concept for the Axio™ product line.  The prospects were in industries such as manufacturing, insurance, banking, government, and finance, in the US, Australia, and Germany.  Over 60% resulted in sales\n",
            "Captured data quality and data migration requirements from customers and constructed integrated data profiling technical and methodology environments\n",
            "Developed training classes in the Axio product line, emphasizing new XML products.  Delivered training to multiple clients\n",
            "\n",
            "Dun and Bradstreet\t\t\t\t\t\t1997-1999\n",
            "Information Consultant\n",
            "Captured customer requirements to develop marketing-centered data marts.  D&B liaison between clients and database/application developers.  Developed and maintained data models and mappings for external clients supporting market research and direct marketing business requirements.  Incorporated multiple source files from multiple companies\n",
            "Integrated multiple D&B industry demographic sources to develop new Market Spectrum Vertical™ pre-packaged data mart solution\n",
            "Maintained internal database that captured transformation rules for client data sources.\n",
            "Satisfied client business goals by implementing integrated data mart solutions using D&B business demographic files\n",
            "\n",
            "American Express\t\t\t\t\t\t1996-1997\n",
            "Data Modeler\n",
            "Contract position (Hired by Volt Technical Services)\n",
            "Developed and maintained data models for new applications and existing systems.  Incorporated and migrated new data sources into existing database infrastructure\n",
            "Developed new data structures, in response to new requirements, into the American Express WorldWide Codes environment for both Operational and Data Warehouse systems\n",
            "Maintained and enhanced the Genesis II system Infrastructure data model for the Global Merchant Services (GMS) division of American Express\n",
            "\n",
            "Xerox Corporation\t\t\t\t\t\t1995-1996\n",
            "Contract position (Hired by TAD Resources International)\n",
            "\tIncorporated new business requirements into a data model in support of the reengineering of the sales compensation process for the United States Customer Organization (USCO)\n",
            "\tDeveloped business rules and data model for the Order Management process in support of the Market to Collections reengineering effort\n",
            "\n",
            "\n",
            "AT&T\t\t\t\t\t\t\t\t1991-1995\n",
            "Data Architect\n",
            "Profiled the current state of the MIS systems architecture for the Global Business Communications Systems (GBCS) business unit to determine the architectural impacts of a Data Warehouse project\n",
            "\tEvaluated metadata repository solutions in support of comprehensive effort by the Global Information Technology Solutions Division of GBCS to assess data resources of the business unit\n",
            "Satisfied business requirements for new client/server sales and back-office processes by developing data transformation specifications for migration from mainframe environment\n",
            "Developed a logical data model, using Oracle CASE Designer and CASE Dictionary, to support the construction of a customer database for the local offices of the GBCS business unit\n",
            "\n",
            "EDUCATION\n",
            "Hofstra University\n",
            "Bachelor of Arts – Mathematics\n",
            "\n",
            "Daniel_Paige_CV-CDA.docx\n",
            "Daniel Paige \n",
            "+1 406-422-2877\n",
            "Chief Data Architect\n",
            "\n",
            "Pathfinding Data Practitioner with extensive global experience delivering Information Management Strategy , Data Intelligence, Data Governance and Big Data.  Expert technical skills in developing and maintaining Data Lakehouse solutions including; data modeling, data mining, system source analysis, integrations, architecture, data visualizations, analytics, database utilities and advanced SQL processes. IT Leader capable of effectively communicating with a diverse community to gather, analyze, negotiate, document requirements and deliver. \n",
            "\n",
            "professional experience\n",
            "\n",
            "AstraZeneca\n",
            "Chief Data Architect – Boston, MA\t09/20 – Present\n",
            "Sr Enterprise Data Architect – Boston, MA\t09/19 – 09/20\n",
            "Evangelizing Data Architecture Strategy across the enterprise \n",
            "Depicting North Star Ecosystems to deliver Just-in-Time Insights\n",
            "Instilling guiding principles, policies and standards\n",
            "Collaborating with and enabling Engineering, Delivery, Governance & Compliance Teams  \n",
            "Defining patterns, reference architecture, frameworks and patterns including; data cataloguing, data privacy & security, DataOps, data domains, data unification…\n",
            "Leading Enterprise Data & Information Architects\n",
            "\n",
            "FIS Global\n",
            "Head of Data Management-APAC – Hong Kong\t03/19 – 09/19\n",
            "Built and lead the Data Management Professional Services Team across APAC \n",
            "Architected and delivered client solutions for actuarial insights and regulatory reporting in the insurance industry\n",
            "\n",
            "Palo IT\t08/18 – 11/18\n",
            "Big Data and Analytics Practice Lead – Hong Kong & Singapore\n",
            "Defined the Data Intelligence service offering for a multinational consulting firm specializing in FinTech\n",
            "Incorporated Data Delivery into DevOps delivery, coaching and training services\n",
            "Managed a team of data practitioners to deliver exceptional services\n",
            "Talent acquisition to support client engagements\n",
            "\n",
            "Intertek\t01/13 – 02/18\n",
            "Enterprise Information Architect – Houston, TX & Shanghai\n",
            "Resided in Shanghai, China and managed the delivery of China mastered data domains to launch new global systems for testing and inspection services.\n",
            "Product and Solution Owner for Big Data using Hadoop, Flume, Sqoop and Elasticsearch and Tableau.  This data lake is just under 1.5PB in size and is sourced from regional hubs, structured, semi-structured and unstructured data.  We mined leveraging optical character recognition (OCR) and machine learning.\n",
            "Architected and implemented Informatica MDM for customer, product, organization, location and reference domains with extensive relational hierarchies.  Integration of numerous source systems including; regional customer hubs, CRM, ERP, billing, lab management and inspection systems. \n",
            "Managed the infrastructure, architecture, design, development, support and maintenance of global data across 128 countries and 22 global business lines servicing clients for auditing, testing and inspection.\n",
            "Managed data architecture, data quality, data governance, data integrations, and business process modeling (BPM), standard operating procedures (SOPs) and change management for the business, systems and processes.\n",
            "Architected and implemented the global enterprise data warehouse.  This is comprised of a myriad of sources/subject areas and is conformed into a holistic insight to enterprise information.  \n",
            "Managed structured, semi-structured and unstructured data across multiple global data centers leveraged by over 500 supported systems.\n",
            "Created guardrails and policies for enterprise data assets while mindfully managing operational efficiencies for the business.    \n",
            "Managed a team of seven (7):  data architects, data modelers, data integrators and analysts responsible for all enterprise information management.\n",
            "DevOps with various technologies within our enterprise ecosystem including; OBIEE, SAP BOBJ/BODS/PowerDesigner, Informatica MDM, IDQ, PowerCenter, ActiveVOS (BPM), MS SQL Server/SSIS/SSAS/Power BI, Tableau, Spotfire, Hadoop, HBase, Elasticsearch, Flume, Sqoop…\n",
            "\n",
            "\n",
            "Professional Services\n",
            "\n",
            "DCP Consulting, Inc.\t09/10 –01/13\n",
            "Senior Information Management Architect\n",
            "\n",
            "Deloitte Consulting, LLP – Denver, CO\t01/08 – 02/09\n",
            "Consulting Manager – Information Management\n",
            "\n",
            "Cognos Corporation – Boston, MA\t06/06 – 01/08\n",
            "Principal Consultant – Business Intelligence / Performance Application\n",
            "\n",
            "Siebel Systems, Inc. – Boston, MA\t03/05 – 06/06\n",
            "Senior Consultant – Business Intelligence Analytics\n",
            "\n",
            "Johns Hopkins University- Baltimore, MD \t08/04 – 03/05\n",
            "Manager - Data Warehouse / Business Intelligence\n",
            "\n",
            "Practice Diagnostic Systems – Boston, MA \t01/00– 09/04\n",
            "Principal Consultant - Business Intelligence \n",
            "\n",
            "\n",
            "Technical Skills\n",
            "\n",
            "education\n",
            "BS in Accounting\n",
            "Southern New Hampshire University\t Manchester, NH\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 2 split resume text\n",
        "import re\n",
        "\n",
        "def segment_text(text, file_name):\n",
        "    # Define keyword groups\n",
        "    keyword_groups = [['Summary', 'PROFILE', 'HIGHLIGHTS', 'Professional Summary', 'PROFESSIONAL SUMMARY'],\n",
        "                      ['Experience', 'Work Experience', 'PROFESSIONAL EXPERIENCE', 'Work History', 'TECHNICAL EXPERIENCE', 'RELATED WORK EXPERIENCE'],\n",
        "                      ['Education', 'EDUCATION', 'Educational Qualification', 'Education & Certification'],\n",
        "                      ['Skills', 'Technical Skills', 'SKILLSET', 'TECHNOLOGY'],\n",
        "                      ['Activities', 'Professional ACTIVITIES', 'ACTIVITIES'],\n",
        "                      ['Projects', 'Other Projects', 'SIDE PROJECTS', 'PROJECTS']]\n",
        "\n",
        "    # Initialize variables\n",
        "    current_category = ''\n",
        "    segmented_text = {'Resume file name': file_name}  # Include file name in segmented text dictionary\n",
        "\n",
        "    # Iterate through lines in text\n",
        "    for line in text.split('\\n'):\n",
        "        # Check if line matches any keyword group\n",
        "        matched = False\n",
        "        for group in keyword_groups:\n",
        "            if any(keyword in line for keyword in group):\n",
        "                current_category = group[0]\n",
        "                segmented_text.setdefault(current_category, '')  # Initialize category if not present\n",
        "                matched = True\n",
        "                break\n",
        "        if not matched and current_category:  # If line doesn't match any keyword and there's a current category\n",
        "            segmented_text[current_category] += line.strip() + '\\n'\n",
        "\n",
        "    return segmented_text\n",
        "\n"
      ],
      "metadata": {
        "id": "yXtSRpM_mMLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openpyxl import load_workbook\n",
        "\n",
        "def write_to_excel(all_segmented_text):\n",
        "    # Load an existing workbook or create a new one if it doesn't exist\n",
        "    wb = load_workbook('Resume Python.xlsx')\n",
        "    # Select the active worksheet or create a new one if it doesn't exist\n",
        "    new_ws = wb.active if 'RawParsed' in wb.sheetnames else wb.create_sheet(title='RawParsed')\n",
        "\n",
        "    # Write headers to the worksheet if the worksheet is empty\n",
        "    if new_ws.max_row == 1:\n",
        "        headers = ['Resume file name', 'Summary', 'Experience', 'Education', 'Skills', 'Activities', 'Projects']\n",
        "        new_ws.append(headers)\n",
        "\n",
        "    # Iterate over each resume's segmented text\n",
        "    for segmented_text in all_segmented_text:\n",
        "        file_name = segmented_text.pop('Resume file name', '')  # Remove and get file name\n",
        "        row_data = [file_name]  # Add file name as the first column\n",
        "        for category in ['Summary', 'Experience', 'Education', 'Skills', 'Activities', 'Projects']:\n",
        "            row_data.append(segmented_text.get(category, '').strip())  # Add category text\n",
        "        new_ws.append(row_data)\n",
        "\n",
        "    # Save the workbook\n",
        "    wb.save(\"Resume Python.xlsx\")\n"
      ],
      "metadata": {
        "id": "LPFtXAs-K4wz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load an existing workbook or create a new one if it doesn't exist\n",
        "wb = load_workbook('Resume Python.xlsx')\n",
        "# Select the RawResume worksheet or create a new one if it doesn't exist\n",
        "ws = wb['RawResume'] if 'RawResume' in wb.sheetnames else wb.create_sheet(title='RawResume')\n",
        "\n",
        "# Clear existing content in RawResume worksheet\n",
        "ws.delete_rows(2, ws.max_row)  # Delete all rows except the header\n",
        "\n",
        "#\n",
        "all_segmented_text = []\n",
        "\n",
        "# Process each file individually\n",
        "for result in results:\n",
        "    file_name, text = result.split('\\n', 1)\n",
        "    segmented_text = segment_text(text, file_name)\n",
        "    all_segmented_text.append(segmented_text)\n",
        "\n",
        "write_to_excel(all_segmented_text)\n"
      ],
      "metadata": {
        "id": "zW_1Or0ud_vH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 3 split job history to each job\n",
        "\n",
        "import datetime\n",
        "\n",
        "# Define regular expression for parsing dates\n",
        "date_regex = r'\\b(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?)\\s+\\d{4}\\b'\n",
        "\n",
        "def write_to_experience(all_segmented_text):\n",
        "    wb = load_workbook('Resume Python.xlsx')\n",
        "    new_ws_2 = wb.active if 'RawExperience' in wb.sheetnames else wb.create_sheet(title='RawExperience')\n",
        "    headers = ['Resume file name', 'Start Date', 'End Date', 'Experience']\n",
        "    new_ws_2.append(headers)\n",
        "\n",
        "    for result in results:\n",
        "      file_name, text = result.split('\\n', 1)\n",
        "      segmented_text = segment_text(text, file_name)\n",
        "      all_segmented_text.append(segmented_text)\n",
        "      print(\"File Name:\", file_name)  # Add this line for debugging\n",
        "      experiences = segmented_text.get('Experience', '').strip().split('\\n')\n",
        "\n",
        "      current_start_date = None\n",
        "      current_end_date = None\n",
        "      current_experience_group = []\n",
        "\n",
        "      for exp in experiences:\n",
        "            # Use regular expression to search for dates\n",
        "            match = re.search(date_regex, exp)\n",
        "            if match:\n",
        "                # If there's a current start date, write the current experience group to the worksheet\n",
        "                if current_start_date:\n",
        "                    new_ws_2.append([file_name, current_start_date, current_end_date, '\\n'.join(current_experience_group)])\n",
        "                    current_experience_group = []\n",
        "                # Set current start date to the matched date\n",
        "                current_start_date = match.group(0)\n",
        "                # Reset current end date\n",
        "                current_end_date = None\n",
        "            # Check if the line contains end date\n",
        "            elif 'End Date' in exp:\n",
        "                end_date = exp.split(':')[1].strip()\n",
        "                if end_date == 'Present':\n",
        "                    current_end_date = datetime.datetime.now().strftime('%m-%d-%Y')\n",
        "                else:\n",
        "                    current_end_date = end_date\n",
        "            # Add the current experience to the current experience group\n",
        "            current_experience_group.append(exp.strip())\n",
        "\n",
        "        # Write the last experience group\n",
        "      if current_start_date:\n",
        "            new_ws_2.append([file_name, current_start_date, current_end_date, '\\n'.join(current_experience_group)])\n",
        "\n",
        "    wb.save(\"Resume Python.xlsx\")\n",
        "\n",
        "# Call the function\n",
        "write_to_experience(all_segmented_text)"
      ],
      "metadata": {
        "id": "swGUtSEOdtXo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "008fb141-46d7-4af2-aebd-796ee5dd41fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File Name: Jayalakshmi+Thiagarajan_Recent.pdf\n",
            "File Name: LEB resume 17 AUG 2021.pdf\n",
            "File Name: Ankush S. -Xendat Collibra Ranger - India.pdf\n",
            "File Name: Anand_Kishore_Resume_EA.docx\n",
            "File Name: Jennifer Morales (2021).docx\n",
            "File Name: bioinfo.pdf\n",
            "File Name: Rahul_Parihar_BI_Consultant_Resume.docx\n",
            "File Name: JoeNovella_Resume-Vertex.docx\n",
            "File Name: Daniel_Paige_CV-CDA.docx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#6-1\n",
        "#Step 4 Extract company name\n",
        "\n",
        "# Open the Excel file\n",
        "wb = load_workbook('Resume Python.xlsx')\n",
        "\n",
        "# Create a new worksheet for company names\n",
        "new_ws_3 = wb.active if 'CompanyName' in wb.sheetnames else wb.create_sheet(title='CompanyName')\n",
        "\n",
        "# Define headers for the worksheet\n",
        "headers = ['Resume file name', 'Start Date', 'End Date', 'Experience', 'CompanyName']\n",
        "\n",
        "# Append headers to the worksheet\n",
        "new_ws_3.append(headers)\n",
        "\n",
        "# Loop through each segmented text\n",
        "for result in results:\n",
        "    file_name, text = result.split('\\n', 1)\n",
        "    segmented_text = segment_text(text, file_name)\n",
        "    experiences = segmented_text.get('Experience', '').strip().split('\\n')\n",
        "    company_names = []\n",
        "\n",
        "    for exp in experiences:\n",
        "        # Mode 1: Check if the line contains a state abbreviation and extract the company name\n",
        "        if re.search(r'\\b(?:AL|AK|AZ|AR|CA|CO|CT|DE|FL|GA|HI|ID|IL|IN|IA|KS|KY|LA|ME|MD|MA|MI|MN|MS|MO|MT|NE|NV|NH|NJ|NM|NY|NC|ND|OH|OK|OR|PA|RI|SC|SD|TN|TX|UT|VT|VA|WA|WV|WI|WY)\\b', exp):\n",
        "            company_names.append(exp.split(',')[-1].strip())\n",
        "\n",
        "        # Mode 2: Check if the line before the date contains the company name\n",
        "        elif 'End Date' in exp:\n",
        "            company_names.append(experiences[experiences.index(exp) - 1].strip())\n",
        "\n",
        "        # Mode 3: Check if the job title and company name are on the same line\n",
        "        elif ',' in exp:\n",
        "            job_title, company_name = exp.split(',', 1)\n",
        "            company_names.append(company_name.strip())\n",
        "\n",
        "        # Mode 4: Check if the line with the date contains the company name\n",
        "        elif re.search(date_regex, exp):\n",
        "            company_names.append(exp.split(',')[0].strip())\n",
        "\n",
        "    # Append the row to the worksheet\n",
        "    for company_name in company_names:\n",
        "        new_ws_3.append([file_name, segmented_text.get('Start Date', ''), segmented_text.get('End Date', ''), '\\n'.join(experiences), company_name])\n",
        "\n",
        "# Save the changes to the Excel file\n",
        "wb.save(\"Resume Python.xlsx\")\n"
      ],
      "metadata": {
        "id": "yVs13I5fmdpb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
